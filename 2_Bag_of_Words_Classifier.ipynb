{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Classifier\n",
    "\n",
    "Here is a bag-of-words classifier for news topics. This model is a simple logistic regression model using bag of words. Before considering whether we need to use a more complex ML algorithm, let's try using a simple classifier for the news data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "\n",
    "from nltk.tokenize.regexp import WordPunctTokenizer\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset provided is malformed JSON. Need to fix up the JSON formatting\n",
    "# so that it can be ingested by pandas.\n",
    "\n",
    "with open('./News_Category_Dataset_v2.json') as file:\n",
    "    lines = file.readlines()\n",
    "    json = f'[{\",\".join(lines)}]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(json, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(df, c):\n",
    "    category_counts = df['category'].value_counts()\n",
    "    min_count = category_counts.min()\n",
    "\n",
    "    # Calculate the probability of keeping a row\n",
    "    # of a given category.\n",
    "    category_probs = (min_count / category_counts) ** (1/c)\n",
    "\n",
    "    # This is a series used to determine the probability that each\n",
    "    # row is kept. Each rows mask depends on its category.\n",
    "    prob_mask = np.zeros(len(df))\n",
    "\n",
    "    for i, category in enumerate(category_counts.index.tolist()):\n",
    "        category_prob = category_probs[i]\n",
    "        category_keep_mask = (df['category'] == category) * category_prob\n",
    "        prob_mask = prob_mask + category_keep_mask\n",
    "\n",
    "    keep_mask = np.random.rand(len(df)) <= prob_mask\n",
    "    \n",
    "    return df[keep_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = downsample(data, c=2)\n",
    "data = data.sample(frac=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "# Special token for tokens that occur MIN_WORD_FFEQ or fewer times in the\n",
    "# entire corpus.\n",
    "__LOW_FREQ_TOKEN__ = '__LOW_FREQ_TOKEN__'\n",
    "\n",
    "MIN_WORD_FREQ = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_and_tokenize_text(text):\n",
    "    cleaned = ''.join([c for c in text if c not in string.punctuation]).lower()\n",
    "    return tokenizer.tokenize(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_rows(df):\n",
    "    tokenized_headlines = df['headline'].apply(cleanup_and_tokenize_text).tolist()\n",
    "    tokenized_desc = df['short_description'].apply(cleanup_and_tokenize_text).tolist()\n",
    "\n",
    "    return [tokens1 + tokens2 for tokens1, tokens2 in zip(tokenized_headlines, tokenized_desc)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unigram_counts(rows):\n",
    "    # Flatten\n",
    "    tokens = [t for tokens in rows for t in tokens]\n",
    "    \n",
    "    counts = {}\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in counts:\n",
    "            counts[token] = 0\n",
    "        counts[token] += 1\n",
    "\n",
    "    return counts\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder_and_decoder(unigram_counts):\n",
    "    encoder = {t:i for i,t in enumerate(unigram_counts.keys())}\n",
    "    decoder = {i:t for t,i in encoder.items()}\n",
    "    \n",
    "    return encoder, decoder\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow_dataframe(encoded_token_rows, encoder, decoder):\n",
    "    bows = np.zeros((len(encoded_token_rows), len(encoder)))\n",
    "\n",
    "    for i, encoded_tokens in enumerate(encoded_token_rows):\n",
    "        for encoded in encoded_tokens:\n",
    "            bows[i, encoded] += 1\n",
    "\n",
    "    columns = [decoder[i] for i in range(len(decoder))]\n",
    "    bows_sparse = sparse.csr_matrix(bows)\n",
    "    df = pd.DataFrame.sparse.from_spmatrix(bows_sparse, columns=columns)\n",
    "    \n",
    "    return df\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/7] Tokenizing rows...\n",
      "[2/7] Generating global unigram count...\n",
      "[3/7] Filtering out low-frequency words...\n",
      "[4/7] Re-computing unigram counts...\n",
      "      NOTE: There are 15364 tokens in the vocab.\n",
      "[5/7] Create encoder / decoder...\n",
      "[6/7] Encoding Token Rows...\n",
      "[7/7] Creating Bag Of Words DataFrame...\n",
      "Done!\n",
      "Ran in 0.36m\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print('[1/7] Tokenizing rows...')\n",
    "token_rows = tokenize_rows(data)\n",
    "\n",
    "print('[2/7] Generating global unigram count...')\n",
    "unigram_counts = create_unigram_counts(token_rows)\n",
    "\n",
    "print('[3/7] Filtering out low-frequency words...')\n",
    "token_rows = [[token if unigram_counts[token] > MIN_WORD_FREQ else __LOW_FREQ_TOKEN__ for token in tokens] for tokens in token_rows]\n",
    "\n",
    "print('[4/7] Re-computing unigram counts...')\n",
    "unigram_counts = create_unigram_counts(token_rows)\n",
    "print(f'      NOTE: There are {len(unigram_counts)} tokens in the vocab.')\n",
    "\n",
    "print('[5/7] Create encoder / decoder...')\n",
    "encoder, decoder = create_encoder_and_decoder(unigram_counts)\n",
    "\n",
    "print('[6/7] Encoding Token Rows...')\n",
    "encoded_token_rows = [[encoder[t] for t in tokens] for tokens in token_rows]\n",
    "\n",
    "print('[7/7] Creating Bag Of Words DataFrame...')\n",
    "data_bow = create_bow_dataframe(encoded_token_rows, encoder, decoder)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Done!')\n",
    "print(f'Ran in {(end_time - start_time)/60:.02f}m')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Ran in 0.00m\n",
      "There are 49449 training samples.\n"
     ]
    }
   ],
   "source": [
    "labels = data['category']\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data_bow, labels, test_size=0.4)\n",
    "X_train = data_bow\n",
    "y_train = labels\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Done!')\n",
    "print(f'Ran in {(end_time - start_time)/60:.02f}m')\n",
    "print(f'There are {len(X_train)} training samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Cross-Validating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validating model 1 / 1 ...\n",
      "Best model is #1\n",
      "Total training time: 229.23m\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "#     LogisticRegression(multi_class='multinomial', solver='lbfgs', C=1e1, max_iter=4000, n_jobs=-1),\n",
    "    LogisticRegression(multi_class='multinomial', solver='lbfgs', C=1e2, max_iter=4000, n_jobs=-1),\n",
    "#     LogisticRegression(multi_class='multinomial', solver='lbfgs', C=1e3, max_iter=4000, n_jobs=-1),\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(f'Training and validating model {i+1} / {len(models)} ...')\n",
    "    model.fit(X_train, y_train)\n",
    "    mean_score = model.score(X_train, y_train)\n",
    "#     mean_score = np.mean(cross_val_score(model, X_train, y_train, cv=5))\n",
    "    scores.append(mean_score)\n",
    "\n",
    "best_model = models[scores.index(max(scores))]\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Best model is #{scores.index(max(scores)) + 1}')\n",
    "print(f'Total training time: {(end_time - start_time)/60:.02f}m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.99%\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train)\n",
    "\n",
    "correct = (predictions == y_train).sum()\n",
    "total = len(predictions)\n",
    "\n",
    "print(f'Train Accuracy: {float(correct)/total*100:.02f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bag_of_words_model.joblib']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(model, 'bag_of_words_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.72688458e-04,  4.58161293e-01, -8.26945159e-02, ...,\n",
       "        -6.03456905e-03, -3.52710947e-02, -1.25955779e-04],\n",
       "       [ 3.32865880e-01,  3.29511216e-01, -7.68876380e-01, ...,\n",
       "        -3.09979825e-04, -1.40003729e-02, -4.12741696e-03],\n",
       "       [ 2.95439415e-02, -3.59347440e-01, -5.04046730e-01, ...,\n",
       "        -2.35206418e-01, -1.89577439e-02, -5.02090778e-03],\n",
       "       ...,\n",
       "       [-2.96556104e-01, -1.38057763e+00,  2.22909983e+00, ...,\n",
       "         2.79526415e-01, -1.31516114e-01, -2.90846068e-03],\n",
       "       [ 7.28923194e-02, -5.57498442e-01,  5.39772366e-01, ...,\n",
       "        -4.79895911e-03, -8.76729519e-04, -6.89602812e-04],\n",
       "       [-1.70823405e-01, -6.24261865e-01, -3.62624733e-01, ...,\n",
       "        -1.02379076e-02, -1.42426855e-01, -3.59644559e-04]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 20423 features per sample; expecting 15364",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-9f2272a42c2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \"\"\"\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 270\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 20423 features per sample; expecting 15364"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "correct = (predictions == y_test).sum()\n",
    "total = len(predictions)\n",
    "\n",
    "print(f'Test Accuracy: {float(correct)/total*100:.02f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
