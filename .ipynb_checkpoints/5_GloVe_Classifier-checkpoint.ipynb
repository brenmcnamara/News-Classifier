{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a version of the News Classifier that uses GloVe vectors for classification. GloVe vectors were downloaded from [here](https://nlp.stanford.edu/projects/glove/). The vectors are stored in the `data` folder of this repo, which is not commited to source code.\n",
    "\n",
    "We implement two models in this notebook: the first uses a simple averaging of word vectors while the other uses a TF-IDF weighting of word vectors.\n",
    "\n",
    "The weighted / non-weighted averaging of word embeddings is then passed into a logistic regression model for class prediction.\n",
    "\n",
    "**Note: There are some random processes within this notebook, so different runs of the notebook may result in different outcomes.**\n",
    "\n",
    "**Note: This notebook assumes the data being loaded has already been randomly shuffled.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from data_utils import WordTokenDataset, WordTokenDatasetSample\n",
    "from time import time\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Setup the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = data_utils.load_embeddings('./data/glove.6B/glove.6B.100d.txt',\n",
    "                                        embedding_dim=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('./data/train_data.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_tokens = {t for t in embeddings.index}\n",
    "dataset = WordTokenDataset(data, accepted_tokens=accepted_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran in 0.30m.\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "dataset.prepare()\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "print(f'Ran in {(end_time - start_time)/60:.02f}m.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a basic hyper-parameter grid search.\n",
    "\n",
    "hyperparams_list = [\n",
    "#     { 'type': 'uniform', 'lr': 0.001,  'batch_size': 100 },\n",
    "#     { 'type': 'uniform', 'lr': 0.01,   'batch_size': 100 },\n",
    "#     { 'type': 'uniform', 'lr': 0.001,  'batch_size': 10  },\n",
    "    { 'type': 'uniform', 'lr': 0.01,   'batch_size': 10  },\n",
    "#     { 'type': 'tf_idf',  'lr': 0.001,  'batch_size': 100 },\n",
    "#     { 'type': 'tf_idf',  'lr': 0.01,   'batch_size': 100 },\n",
    "#     { 'type': 'tf_idf',  'lr': 0.001,  'batch_size': 10  },\n",
    "#     { 'type': 'tf_idf',  'lr': 0.01,   'batch_size': 10  },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, embeddings, n_classes, weighting='uniform'):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.embedding_bag = torch.nn.EmbeddingBag.from_pretrained(embeddings.clone(), mode='sum')\n",
    "        self.linear = torch.nn.Linear(self.embedding_bag.embedding_dim, n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, samples):\n",
    "        weights = samples.create_uniform_weights()\n",
    "        x = self.embedding_bag(samples.sequence, samples.offset, per_sample_weights=weights)\n",
    "        output = self.linear(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def predict(self, samples):\n",
    "        with torch.no_grad():\n",
    "            weights = samples.create_uniform_weights()\n",
    "            outputs = self(samples.sequence, samples.offset, weights)\n",
    "            predictions = torch.argmax(outputs, axis=1)\n",
    "\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, dataset, data_loader, epochs, log=True):\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "\n",
    "        for samples in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(samples)\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss)\n",
    "\n",
    "        train_loss = torch.mean(torch.stack(losses))\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        if log and (epoch + 1) % 1 == 0:\n",
    "            train_loss_estimator_size = 1000\n",
    "            train_loss_estimator_start = max(1, len(dataset) - train_loss_estimator_size)\n",
    "            random_start = torch.randint(high=train_loss_estimator_start)\n",
    "\n",
    "            samples = dataset[random_start:(random_start+train_loss_estimator_size)]\n",
    "            predictions = model.predict(samples)\n",
    "            labels = samples['label']\n",
    "\n",
    "            total = len(labels)\n",
    "            correct = torch.sum(labels == predictions)\n",
    "\n",
    "            print(f'Epoch {epoch + 1}')\n",
    "            print(f'Accuracy: {float(correct)/total*100:.02f}%.')\n",
    "            print(f'Training Loss: {train_loss.item()}')\n",
    "            print()\n",
    "        \n",
    "    return train_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training Model 1 / 8...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2d29eecffd0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# 1. Setup Data Loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'weights'"
     ]
    }
   ],
   "source": [
    "# Non-Weighted Model Training.\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "models = []\n",
    "train_losses_list = []\n",
    "valid_losses = []\n",
    "\n",
    "accepted_tokens = {t for t in embeddings.index}\n",
    "\n",
    "for i, hyperparams in enumerate(hyperparams_list):\n",
    "    print(f'Starting training Model {i+1} / {len(hyperparams_list)}...')\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    batch_size = hyperparams['batch_size']\n",
    "    lr = hyperparams['lr']\n",
    "\n",
    "    # 1. Setup Data Loader\n",
    "\n",
    "    dataset = WordTokenDataset(data, accepted_tokens=accepted_tokens)\n",
    "\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False,\n",
    "                             collate_fn=data_utils.collate_samples)\n",
    "\n",
    "    # 2. Create the Model\n",
    "\n",
    "    model = Model(embeddings=embeddings, n_classes=len(label_types))\n",
    "\n",
    "    # 3. Setup Criterion and Optimizer\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # 4. Train the Model\n",
    "\n",
    "    train_losses = train(model,\n",
    "                         criterion,\n",
    "                         optimizer,\n",
    "                         dataset,\n",
    "                         data_loader,\n",
    "                         epochs)\n",
    "    \n",
    "    # 5. Calculate Validation Loss\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_dataset = WordTokenDataset(valid_sequence, valid_offsets, valid_labels, valid_weights)\n",
    "        valid_samples = valid_dataset.all_samples()\n",
    "\n",
    "        outputs = model(valid_samples)\n",
    "\n",
    "        valid_loss = criterion(outputs, valid_labels)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "    end_time = time()\n",
    "\n",
    "    models.append(model)\n",
    "    train_losses_list.append(train_losses)\n",
    "\n",
    "    print(f'Model completed in {(end_time - start_time)/60:.02f}m.')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_mask = [hp['type'] == 'uniform' for hp in hyperparams_list]\n",
    "\n",
    "uniform_models = [m for i, m in enumerate(models) if uniform_mask[i]]\n",
    "uniform_train_losses_list = [losses for i, losses in enumerate(train_losses_list) if uniform_mask[i]]\n",
    "uniform_valid_losses = [loss.item() for i, loss in enumerate(valid_losses) if uniform_mask[i]]\n",
    "\n",
    "tf_idf_models = [m for i, m in enumerate(models) if not uniform_mask[i]]\n",
    "tf_idf_train_losses_list = [losses for i, losses in enumerate(train_losses_list) if not uniform_mask[i]]\n",
    "tf_idf_valid_losses = [loss.item() for i, loss in enumerate(valid_losses) if not uniform_mask[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss of uniform models.\n",
    "\n",
    "for i, model in enumerate(uniform_models):\n",
    "    train_losses = uniform_train_losses_list[i]\n",
    "    plt.plot(train_losses)\n",
    "\n",
    "plt.legend([f'Model {i+1}' for i in range(len(uniform_models))])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss of tf-idf models.\n",
    "\n",
    "for i, model in enumerate(tf_idf_models):\n",
    "    train_losses = tf_idf_train_losses_list[i]\n",
    "    plt.plot(train_losses)\n",
    "\n",
    "plt.legend([f'Model {i+1}' for i in range(len(tf_idf_models))])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Best Models\n",
    "\n",
    "We will grab the \"best\" model trained on uniform weights and tf-idf weights. This will be based on which model scores the lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the best models.\n",
    "\n",
    "best_uniform_model_idx = uniform_valid_losses.index(min(uniform_valid_losses))\n",
    "best_uniform_model = uniform_models[best_uniform_model_idx]\n",
    "\n",
    "best_tf_idf_model_idx = tf_idf_valid_losses.index(min(tf_idf_valid_losses))\n",
    "best_tf_idf_model = tf_idf_models[best_tf_idf_model_idx]\n",
    "\n",
    "print(f'Best Uniform Model: {best_uniform_model_idx+1}')\n",
    "print(f'Best TF-IDF Model:  {best_tf_idf_model_idx+1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_dataset = DocDataset(valid_sequence, valid_offsets, valid_labels, valid_uniform_weights)\n",
    "\n",
    "uniform_samples = uniform_dataset.all_samples()\n",
    "\n",
    "uniform_predictions = best_uniform_model.predict(uniform_samples['data'], uniform_samples['offset'], uniform_samples['weights'])\n",
    "\n",
    "total = len(uniform_samples['label'])\n",
    "correct = torch.sum(uniform_predictions == uniform_samples['label'])\n",
    "\n",
    "print(f'Accuracy of Uniform Model: {(float(correct) / total)*100:.02f}%.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dataset = DocDataset(valid_sequence, valid_offsets, valid_labels, valid_tf_idf_weights)\n",
    "\n",
    "tf_idf_samples = tf_idf_dataset.all_samples()\n",
    "\n",
    "tf_idf_predictions = best_tf_idf_model.predict(tf_idf_samples['data'], tf_idf_samples['offset'], tf_idf_samples['weights'])\n",
    "\n",
    "total = len(tf_idf_samples['label'])\n",
    "correct = torch.sum(tf_idf_predictions == tf_idf_samples['label'])\n",
    "\n",
    "print(f'Accuracy of TF-IDF Model: {(float(correct) / total)*100:.02f}%.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(labels, predictions):\n",
    "    # Displaying a confusion matrix of the validation results for our model.\n",
    "\n",
    "    categories = labels.unique()\n",
    "    category_encoder = { c.item():i for i,c in enumerate(categories) }\n",
    "\n",
    "    confusion_matrix = np.random.rand(len(categories), len(categories))\n",
    "\n",
    "    for i, category in enumerate(categories):\n",
    "        row = np.zeros(len(categories))\n",
    "\n",
    "        cat_mask = (labels == category.item()).tolist()\n",
    "        cat_preds = predictions[cat_mask]\n",
    "        \n",
    "        for category in categories:\n",
    "            pred_count = torch.sum(cat_preds == category.item())\n",
    "            row[category_encoder[category.item()]] = pred_count\n",
    "            \n",
    "        confusion_matrix[i, :] = row / len(cat_preds)\n",
    "\n",
    "    return confusion_matrix, category_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    cax = ax.matshow(confusion_matrix)\n",
    "\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Uniform Model\n",
    "\n",
    "uniform_confusion_matrix, category_encoder = create_confusion_matrix(valid_labels, uniform_predictions)\n",
    "show_confusion_matrix(uniform_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for TF-IDF Model\n",
    "\n",
    "tf_idf_confusion_matrix, category_encoder = create_confusion_matrix(valid_labels, tf_idf_predictions)\n",
    "show_confusion_matrix(tf_idf_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_uniform_model.state_dict(), './models/uniform_glove_model.torch')\n",
    "torch.save(best_tf_idf_model.state_dict(), './models/tf_idf_model.torch')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
