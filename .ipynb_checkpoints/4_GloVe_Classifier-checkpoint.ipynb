{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a version of the News Classifier that uses GloVe vectors for classification. GloVe vectors were downloaded from [here](https://nlp.stanford.edu/projects/glove/). The vectors are stored in the `data` folder of this repo, which is not commited to source code.\n",
    "\n",
    "We implement two models in this notebook: the first uses a simple averaging of word vectors while the other uses a TF-IDF weighting of word vectors.\n",
    "\n",
    "The weighted / non-weighted averaging of word embeddings is then passed into a logistic regression model for class prediction.\n",
    "\n",
    "**Note: There are some random processes within this notebook, so different runs of the notebook may result in different outcomes.**\n",
    "\n",
    "**Note: This notebook assumes the data being loaded has already been randomly shuffled.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from time import time\n",
    "from torch.utils.data import BatchSampler, Dataset, DataLoader, SequentialSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Setup the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran in 0.56m.\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "data = pd.read_pickle('./data/train_data_glove.pickle')\n",
    "labels = pd.read_pickle('./data/train_labels.pickle')\n",
    "word_embeddings = pd.read_pickle('./data/embeddings.50d.pickle')\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "print(f'Ran in {(end_time - start_time)/60:.02f}m.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are using the word embeddings provided,\n",
    "# we can't make use of the __LOW_FREQ_TOKEN__ and\n",
    "# __UNK__ tokens. Drop those columns.\n",
    "\n",
    "data = data.drop(columns=['__LOW_FREQ_TOKEN__', '__UNK__'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with no words.\n",
    "non_zero_mask = data.sum(axis=1) != 0\n",
    "\n",
    "data = data[non_zero_mask]\n",
    "labels = labels[non_zero_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_encoder = {w:i for i,w in enumerate(word_embeddings.index)}\n",
    "word_decoder = {i:w for w,i in word_encoder.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will sort the columns in the data to match the\n",
    "# encoding of the word embeddings. This will make\n",
    "# things easier later on.\n",
    "\n",
    "data = data[sorted(data.columns, key=lambda word: word_encoder[word])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_types = {l for l in labels}\n",
    "\n",
    "label_encoder = {l:i for i, l in enumerate(label_types)}\n",
    "label_decoder = {i:l for l, i in label_encoder.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.LongTensor(data.values)\n",
    "labels = torch.LongTensor([label_encoder[l] for l in labels])\n",
    "embeddings = torch.FloatTensor(word_embeddings.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is in bag-of-words format and our model expects a\n",
    "# sequence of word indices. Need to convert the type here.\n",
    "\n",
    "def bow_to_sequence(data_bow, encoder):\n",
    "    \"\"\"\n",
    "    Converts a tensor using the bag-of-words data format to two\n",
    "    tensors: the first tensor is a flat sequence of characters,\n",
    "    the second tensor is a sequence of offsets to split the data on.\n",
    "    \n",
    "    Note that we are assuming the columns of data_bow\n",
    "    match the indices of the encoder.\n",
    "    \"\"\"\n",
    "    word_counts = data_bow.sum(axis=1)\n",
    "\n",
    "    offsets = torch.zeros_like(word_counts, dtype=torch.int64)\n",
    "    for i, count in enumerate(word_counts[:-1]):\n",
    "        offsets[i+1] = offsets[i] + count\n",
    "        \n",
    "    sequence_len = word_counts.sum()\n",
    "    sequence = torch.zeros(sequence_len, dtype=torch.int64)\n",
    "    term_freqs = torch.zeros(sequence_len, dtype=torch.int64)\n",
    "\n",
    "    non_zero_locations = torch.nonzero(data_bow)\n",
    "\n",
    "    # Maps a word idx to the number of documents that index\n",
    "    # shows up.\n",
    "    doc_freq_map = {}\n",
    "\n",
    "    # Note: This iteration assumes that non_zero_locations\n",
    "    # has locations sorted by rows.\n",
    "    iter = 0\n",
    "    for location in non_zero_locations:\n",
    "        row = location[0]\n",
    "        col = location[1]\n",
    "        freq = data_bow[row, col].item()\n",
    "\n",
    "        if col.item() in doc_freq_map:\n",
    "            doc_freq_map[col.item()] += 1\n",
    "        else:\n",
    "            doc_freq_map[col.item()] = 1\n",
    "\n",
    "        for _ in range(freq):\n",
    "            sequence[iter] = col\n",
    "            term_freqs[iter] = freq\n",
    "            iter +=1\n",
    "    \n",
    "    return sequence, offsets, term_freqs, doc_freq_map\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran in 0.99m.\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "data_sequence, data_offsets, data_freqs, doc_freq_map = bow_to_sequence(data, word_encoder)\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "print(f'Ran in {(end_time - start_time)/60:.02f}m.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF weighting has been shown to out-perform a simple averaging of word vectors when passing them into a language model. Here, we calculate TF-IDF weighting of our words to generate weighted averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf_idf_weights(sequence, offsets, term_freqs, doc_freq_map):\n",
    "    \"\"\"\n",
    "    Creates weights for sequences based on tf-idf weighting scheme.\n",
    "    \"\"\"\n",
    "    n_docs = len(offsets)\n",
    "\n",
    "    doc_freq_as_sequence = torch.FloatTensor([doc_freq_map[word_idx.item()] for word_idx in sequence])\n",
    "\n",
    "    idf = torch.log(n_docs / doc_freq_as_sequence)\n",
    "\n",
    "    tf_idf = term_freqs * idf\n",
    "    \n",
    "    # Need to normalize the weights so all samples add up to 1.\n",
    "    offsets_with_end = torch.cat([offsets, torch.LongTensor([len(sequence)])])\n",
    "    \n",
    "    for i, offset in enumerate(offsets_with_end[:-1]):\n",
    "        start = offset.item()\n",
    "        end = offsets_with_end[i+1].item()\n",
    "        \n",
    "        weights = tf_idf[start:end]\n",
    "        total_weight = torch.sum(weights)\n",
    "        tf_idf[start:end] = weights / total_weight\n",
    "        \n",
    "    return tf_idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_uniform_weights(sequence, offsets):\n",
    "    \"\"\"\n",
    "    Creates uniform weighting scheme for words.\n",
    "    \"\"\"\n",
    "    offsets_with_end = torch.cat([offsets, torch.LongTensor([len(sequence)])])\n",
    "\n",
    "    weights = torch.zeros_like(sequence, dtype=torch.float32)\n",
    "\n",
    "    for i, offset in enumerate(offsets_with_end[:-1]):\n",
    "        start = offset.item()\n",
    "        end = offsets_with_end[i+1].item()\n",
    "\n",
    "        seq_len = end - start\n",
    "        weights[start:end] = 1. / seq_len\n",
    "        \n",
    "    return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran in 0.02m.\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "uniform_weights = calculate_uniform_weights(data_sequence,\n",
    "                                            data_offsets)\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "print(f'Ran in {(end_time - start_time)/60:.02f}m.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran in 0.12m.\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "tf_idf_weights = calculate_tf_idf_weights(data_sequence,\n",
    "                                          data_offsets,\n",
    "                                          data_freqs,\n",
    "                                          doc_freq_map)\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "print(f'Ran in {(end_time - start_time)/60:.02f}m.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocDataset(Dataset):\n",
    "    def __init__(self, sequence, offsets, labels, weights):\n",
    "        self.sequence = sequence\n",
    "        self.offsets = offsets\n",
    "        self.labels = labels\n",
    "        self.weights = weights\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.offsets)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        offset = self.offsets[idx]\n",
    "\n",
    "        if (idx + 1) < len(self):\n",
    "            end = self.offsets[idx + 1]\n",
    "            data_slice = slice(offset, end)\n",
    "        else:\n",
    "            data_slice = slice(offset)\n",
    "\n",
    "        data = self.sequence[data_slice]\n",
    "        weights = self.weights[data_slice]\n",
    "\n",
    "        return {'data': data,\n",
    "                'label': self.labels[idx],\n",
    "                'offset': offset,\n",
    "                'weights': weights}\n",
    "\n",
    "\n",
    "    def all_samples(self):\n",
    "        return {'data': self.sequence,\n",
    "                'label': self.labels,\n",
    "                'offset': self.offsets,\n",
    "                'weights': self.weights}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_samples(samples):\n",
    "    \"\"\"\n",
    "    Samples are collated into a format that can be consumed\n",
    "    more easily by the model. A sample has a data, label,\n",
    "    offset key, which are combined into the same format.\n",
    "    \n",
    "    Note: This method assumes that samples are in\n",
    "    sequential order.\n",
    "    \"\"\"\n",
    "    data = torch.cat([s['data'] for s in samples])\n",
    "    \n",
    "    label = torch.LongTensor([s['label'].item() for s in samples])\n",
    "\n",
    "    offset = torch.LongTensor([s['offset'].item() for s in samples])\n",
    "    offset = offset - offset[0]\n",
    "    \n",
    "    weights = torch.cat([s['weights'] for s in samples])\n",
    "\n",
    "    return {'data': data,\n",
    "            'label': label,\n",
    "            'offset': offset,\n",
    "            'weights': weights}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training / validation sets.\n",
    "\n",
    "train_fraction = 0.85\n",
    "split_idx = math.floor(len(data_offsets) * train_fraction)\n",
    "\n",
    "train_offsets = data_offsets[:split_idx]\n",
    "valid_offsets = data_offsets[split_idx:]\n",
    "\n",
    "train_sequence = data_sequence[:valid_offsets[0]]\n",
    "valid_sequence = data_sequence[valid_offsets[0]:]\n",
    "\n",
    "train_labels = labels[:split_idx]\n",
    "valid_labels = labels[split_idx:]\n",
    "\n",
    "train_uniform_weights = uniform_weights[:valid_offsets[0]]\n",
    "valid_uniform_weights = uniform_weights[valid_offsets[0]:]\n",
    "\n",
    "train_tf_idf_weights = tf_idf_weights[:valid_offsets[0]]\n",
    "valid_tf_idf_weights = tf_idf_weights[valid_offsets[0]:]\n",
    "\n",
    "# Need to re-calibrate the offsets of the validation set so\n",
    "# the first offset points to the beginning of the\n",
    "# validation sequence.\n",
    "valid_offsets = valid_offsets - valid_offsets[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a basic hyper-parameter grid search.\n",
    "\n",
    "hyperparams_list = [\n",
    "    { 'type': 'uniform', 'lr': 0.001,  'batch_size': 100, 'weights': (train_uniform_weights, valid_uniform_weights) },\n",
    "    { 'type': 'uniform', 'lr': 0.01,   'batch_size': 100, 'weights': (train_uniform_weights, valid_uniform_weights) },\n",
    "    { 'type': 'uniform', 'lr': 0.001,  'batch_size': 10,  'weights': (train_uniform_weights, valid_uniform_weights) },\n",
    "    { 'type': 'uniform', 'lr': 0.01,   'batch_size': 10,  'weights': (train_uniform_weights, valid_uniform_weights) },\n",
    "    { 'type': 'tf_idf',  'lr': 0.001,  'batch_size': 100, 'weights': (train_tf_idf_weights,  valid_tf_idf_weights)  },\n",
    "    { 'type': 'tf_idf',  'lr': 0.01,   'batch_size': 100, 'weights': (train_tf_idf_weights,  valid_tf_idf_weights)  },\n",
    "    { 'type': 'tf_idf',  'lr': 0.001,  'batch_size': 10,  'weights': (train_tf_idf_weights,  valid_tf_idf_weights)  },\n",
    "    { 'type': 'tf_idf',  'lr': 0.01,   'batch_size': 10,  'weights': (train_tf_idf_weights,  valid_tf_idf_weights)  },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, embeddings, n_classes):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.embedding_bag = torch.nn.EmbeddingBag.from_pretrained(embeddings.clone(), mode='sum')\n",
    "        self.linear = torch.nn.Linear(self.embedding_bag.embedding_dim, n_classes)\n",
    "        \n",
    "    def forward(self, input, offset, weights):\n",
    "        if weights is not None:\n",
    "            x = self.embedding_bag(input, offset, per_sample_weights=weights)\n",
    "        else:\n",
    "            x = self.embedding_bag(input, offset, per_sample_weights=weights)\n",
    "\n",
    "        output = self.linear(x)\n",
    "        return output\n",
    "    \n",
    "    def predict(self, input, offset, weights):\n",
    "        with torch.no_grad():\n",
    "            outputs = self(input, offset, weights)\n",
    "            predictions = torch.argmax(outputs, axis=1)\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, dataset, data_loader, epochs, log=True):\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "\n",
    "        for batch in data_loader:\n",
    "            data = batch['data']\n",
    "            label = batch['label']\n",
    "            offset = batch['offset']\n",
    "            weights = batch['weights']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data, offset, weights)\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss)\n",
    "\n",
    "        train_loss = torch.mean(torch.stack(losses))\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        if log and (epoch + 1) % 100 == 0:\n",
    "            samples = dataset.all_samples()\n",
    "            predictions = model.predict(samples['data'], samples['offset'], samples['weights'])\n",
    "            labels = samples['label']\n",
    "\n",
    "            total = len(labels)\n",
    "            correct = torch.sum(labels == predictions)\n",
    "\n",
    "            print(f'Epoch {epoch + 1}')\n",
    "            print(f'Accuracy: {float(correct)/total*100:.02f}%.')\n",
    "            print(f'Training Loss: {train_loss.item()}')\n",
    "            print()\n",
    "        \n",
    "    return train_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training Model 1 / 8...\n",
      "Epoch 100\n",
      "Accuracy: 41.88%.\n",
      "Training Loss: 2.1149163246154785\n",
      "\n",
      "Epoch 200\n",
      "Accuracy: 41.98%.\n",
      "Training Loss: 2.1063008308410645\n",
      "\n",
      "Epoch 300\n",
      "Accuracy: 42.00%.\n",
      "Training Loss: 2.10298752784729\n",
      "\n",
      "Epoch 400\n",
      "Accuracy: 42.08%.\n",
      "Training Loss: 2.1010093688964844\n",
      "\n",
      "Epoch 500\n",
      "Accuracy: 42.13%.\n",
      "Training Loss: 2.0994534492492676\n",
      "\n",
      "Epoch 600\n",
      "Accuracy: 42.13%.\n",
      "Training Loss: 2.0984957218170166\n",
      "\n",
      "Epoch 700\n",
      "Accuracy: 42.14%.\n",
      "Training Loss: 2.097522020339966\n",
      "\n",
      "Epoch 800\n",
      "Accuracy: 42.11%.\n",
      "Training Loss: 2.0966522693634033\n",
      "\n",
      "Epoch 900\n",
      "Accuracy: 42.13%.\n",
      "Training Loss: 2.0959839820861816\n",
      "\n",
      "Epoch 1000\n",
      "Accuracy: 42.16%.\n",
      "Training Loss: 2.0953643321990967\n",
      "\n",
      "Epoch 1100\n",
      "Accuracy: 42.18%.\n",
      "Training Loss: 2.095696210861206\n",
      "\n",
      "Epoch 1200\n",
      "Accuracy: 42.21%.\n",
      "Training Loss: 2.094327688217163\n",
      "\n",
      "Epoch 1300\n",
      "Accuracy: 42.24%.\n",
      "Training Loss: 2.0938496589660645\n",
      "\n",
      "Epoch 1400\n",
      "Accuracy: 42.24%.\n",
      "Training Loss: 2.093353748321533\n",
      "\n",
      "Epoch 1500\n",
      "Accuracy: 42.26%.\n",
      "Training Loss: 2.0932886600494385\n",
      "\n",
      "Epoch 1600\n",
      "Accuracy: 42.27%.\n",
      "Training Loss: 2.092564582824707\n",
      "\n",
      "Epoch 1700\n",
      "Accuracy: 42.26%.\n",
      "Training Loss: 2.092254638671875\n",
      "\n",
      "Epoch 1800\n",
      "Accuracy: 42.28%.\n",
      "Training Loss: 2.092055082321167\n",
      "\n",
      "Epoch 1900\n",
      "Accuracy: 42.32%.\n",
      "Training Loss: 2.091718912124634\n",
      "\n",
      "Epoch 2000\n",
      "Accuracy: 42.30%.\n",
      "Training Loss: 2.0914595127105713\n",
      "\n",
      "Model completed in 54.93m.\n",
      "\n",
      "Starting training Model 2 / 8...\n",
      "Epoch 100\n",
      "Accuracy: 41.51%.\n",
      "Training Loss: 2.1190993785858154\n",
      "\n",
      "Epoch 200\n",
      "Accuracy: 41.28%.\n",
      "Training Loss: 2.1239962577819824\n",
      "\n",
      "Epoch 300\n",
      "Accuracy: 41.55%.\n",
      "Training Loss: 2.116609573364258\n",
      "\n",
      "Epoch 400\n",
      "Accuracy: 41.63%.\n",
      "Training Loss: 2.114166498184204\n",
      "\n",
      "Epoch 500\n",
      "Accuracy: 41.72%.\n",
      "Training Loss: 2.1118381023406982\n",
      "\n",
      "Epoch 600\n",
      "Accuracy: 41.78%.\n",
      "Training Loss: 2.111124277114868\n",
      "\n",
      "Epoch 700\n",
      "Accuracy: 41.69%.\n",
      "Training Loss: 2.1109073162078857\n",
      "\n",
      "Epoch 800\n",
      "Accuracy: 41.48%.\n",
      "Training Loss: 2.1164908409118652\n",
      "\n",
      "Epoch 900\n",
      "Accuracy: 41.41%.\n",
      "Training Loss: 2.117283582687378\n",
      "\n",
      "Epoch 1000\n",
      "Accuracy: 41.57%.\n",
      "Training Loss: 2.1141650676727295\n",
      "\n",
      "Epoch 1100\n",
      "Accuracy: 41.66%.\n",
      "Training Loss: 2.1122772693634033\n",
      "\n",
      "Epoch 1200\n",
      "Accuracy: 41.74%.\n",
      "Training Loss: 2.1115355491638184\n",
      "\n",
      "Epoch 1300\n",
      "Accuracy: 41.67%.\n",
      "Training Loss: 2.11130690574646\n",
      "\n",
      "Epoch 1400\n",
      "Accuracy: 41.75%.\n",
      "Training Loss: 2.110206127166748\n",
      "\n",
      "Epoch 1500\n",
      "Accuracy: 41.79%.\n",
      "Training Loss: 2.1102073192596436\n",
      "\n",
      "Epoch 1600\n",
      "Accuracy: 41.73%.\n",
      "Training Loss: 2.110034465789795\n",
      "\n",
      "Epoch 1700\n",
      "Accuracy: 41.77%.\n",
      "Training Loss: 2.109825849533081\n",
      "\n",
      "Epoch 1800\n",
      "Accuracy: 41.78%.\n",
      "Training Loss: 2.1099302768707275\n",
      "\n",
      "Epoch 1900\n",
      "Accuracy: 41.77%.\n",
      "Training Loss: 2.109661102294922\n",
      "\n",
      "Epoch 2000\n",
      "Accuracy: 41.81%.\n",
      "Training Loss: 2.1098735332489014\n",
      "\n",
      "Model completed in 54.96m.\n",
      "\n",
      "Starting training Model 3 / 8...\n",
      "Epoch 100\n",
      "Accuracy: 41.87%.\n",
      "Training Loss: 2.108637809753418\n",
      "\n",
      "Epoch 200\n",
      "Accuracy: 41.98%.\n",
      "Training Loss: 2.1040260791778564\n",
      "\n",
      "Epoch 300\n",
      "Accuracy: 42.00%.\n",
      "Training Loss: 2.1024322509765625\n",
      "\n",
      "Epoch 400\n",
      "Accuracy: 41.97%.\n",
      "Training Loss: 2.100294828414917\n",
      "\n",
      "Epoch 500\n",
      "Accuracy: 42.00%.\n",
      "Training Loss: 2.0989463329315186\n",
      "\n",
      "Epoch 600\n",
      "Accuracy: 42.03%.\n",
      "Training Loss: 2.0979347229003906\n",
      "\n",
      "Epoch 700\n",
      "Accuracy: 42.02%.\n",
      "Training Loss: 2.097195625305176\n",
      "\n",
      "Epoch 800\n",
      "Accuracy: 42.06%.\n",
      "Training Loss: 2.096531629562378\n",
      "\n",
      "Epoch 900\n",
      "Accuracy: 42.15%.\n",
      "Training Loss: 2.0959107875823975\n",
      "\n",
      "Epoch 1000\n",
      "Accuracy: 42.07%.\n",
      "Training Loss: 2.156829595565796\n",
      "\n",
      "Epoch 1100\n",
      "Accuracy: 42.11%.\n",
      "Training Loss: 2.0952539443969727\n",
      "\n",
      "Epoch 1200\n",
      "Accuracy: 42.11%.\n",
      "Training Loss: 2.0952224731445312\n",
      "\n",
      "Epoch 1300\n",
      "Accuracy: 42.13%.\n",
      "Training Loss: 2.094921588897705\n",
      "\n",
      "Epoch 1400\n",
      "Accuracy: 42.14%.\n",
      "Training Loss: 2.0947694778442383\n",
      "\n",
      "Epoch 1500\n",
      "Accuracy: 42.15%.\n",
      "Training Loss: 2.094529628753662\n",
      "\n",
      "Epoch 1600\n",
      "Accuracy: 42.10%.\n",
      "Training Loss: 2.127986431121826\n",
      "\n",
      "Epoch 1700\n",
      "Accuracy: 42.19%.\n",
      "Training Loss: 2.0947887897491455\n",
      "\n",
      "Epoch 1800\n",
      "Accuracy: 42.12%.\n",
      "Training Loss: 2.0940451622009277\n",
      "\n",
      "Epoch 1900\n",
      "Accuracy: 42.15%.\n",
      "Training Loss: 2.094160318374634\n",
      "\n",
      "Epoch 2000\n",
      "Accuracy: 42.16%.\n",
      "Training Loss: 2.094141960144043\n",
      "\n",
      "Model completed in 123.18m.\n",
      "\n",
      "Starting training Model 4 / 8...\n",
      "Epoch 100\n",
      "Accuracy: 40.68%.\n",
      "Training Loss: 2.168327569961548\n",
      "\n",
      "Epoch 200\n",
      "Accuracy: 40.69%.\n",
      "Training Loss: 2.166490316390991\n",
      "\n",
      "Epoch 300\n",
      "Accuracy: 40.46%.\n",
      "Training Loss: 2.175846576690674\n",
      "\n",
      "Epoch 400\n",
      "Accuracy: 40.38%.\n",
      "Training Loss: 2.260843515396118\n",
      "\n",
      "Epoch 500\n",
      "Accuracy: 40.73%.\n",
      "Training Loss: 2.1656925678253174\n",
      "\n",
      "Epoch 600\n",
      "Accuracy: 40.57%.\n",
      "Training Loss: 2.3501229286193848\n",
      "\n",
      "Epoch 700\n",
      "Accuracy: 40.63%.\n",
      "Training Loss: 2.165236473083496\n",
      "\n",
      "Epoch 800\n",
      "Accuracy: 40.72%.\n",
      "Training Loss: 2.165390729904175\n",
      "\n",
      "Epoch 900\n",
      "Accuracy: 40.37%.\n",
      "Training Loss: 2.2593491077423096\n",
      "\n",
      "Epoch 1000\n",
      "Accuracy: 40.75%.\n",
      "Training Loss: 2.164201498031616\n",
      "\n",
      "Epoch 1100\n",
      "Accuracy: 40.45%.\n",
      "Training Loss: 2.182559013366699\n",
      "\n",
      "Epoch 1200\n",
      "Accuracy: 40.76%.\n",
      "Training Loss: 2.164179801940918\n",
      "\n",
      "Epoch 1300\n",
      "Accuracy: 40.38%.\n",
      "Training Loss: 2.3658859729766846\n",
      "\n",
      "Epoch 1400\n",
      "Accuracy: 40.77%.\n",
      "Training Loss: 2.164400100708008\n",
      "\n",
      "Epoch 1500\n",
      "Accuracy: 40.76%.\n",
      "Training Loss: 2.1646313667297363\n",
      "\n",
      "Epoch 1600\n",
      "Accuracy: 40.75%.\n",
      "Training Loss: 2.1646833419799805\n",
      "\n",
      "Epoch 1700\n",
      "Accuracy: 40.75%.\n",
      "Training Loss: 2.1649982929229736\n",
      "\n",
      "Epoch 1800\n",
      "Accuracy: 40.75%.\n",
      "Training Loss: 2.165051221847534\n",
      "\n",
      "Epoch 1900\n",
      "Accuracy: 40.74%.\n",
      "Training Loss: 2.1652204990386963\n",
      "\n",
      "Epoch 2000\n",
      "Accuracy: 40.74%.\n",
      "Training Loss: 2.1655526161193848\n",
      "\n",
      "Model completed in 121.91m.\n",
      "\n",
      "Starting training Model 5 / 8...\n",
      "Epoch 100\n",
      "Accuracy: 40.75%.\n",
      "Training Loss: 2.151698112487793\n",
      "\n",
      "Epoch 200\n",
      "Accuracy: 40.75%.\n",
      "Training Loss: 2.1484177112579346\n",
      "\n",
      "Epoch 300\n",
      "Accuracy: 40.85%.\n",
      "Training Loss: 2.1463842391967773\n",
      "\n",
      "Epoch 400\n",
      "Accuracy: 40.85%.\n",
      "Training Loss: 2.1449944972991943\n",
      "\n",
      "Epoch 500\n",
      "Accuracy: 40.87%.\n",
      "Training Loss: 2.144284725189209\n",
      "\n",
      "Epoch 600\n",
      "Accuracy: 40.93%.\n",
      "Training Loss: 2.1433663368225098\n",
      "\n",
      "Epoch 700\n",
      "Accuracy: 40.93%.\n",
      "Training Loss: 2.142597198486328\n",
      "\n",
      "Epoch 800\n",
      "Accuracy: 40.82%.\n",
      "Training Loss: 2.1440491676330566\n",
      "\n",
      "Epoch 900\n",
      "Accuracy: 40.98%.\n",
      "Training Loss: 2.141726016998291\n",
      "\n",
      "Epoch 1000\n",
      "Accuracy: 40.99%.\n",
      "Training Loss: 2.1411397457122803\n",
      "\n",
      "Epoch 1100\n",
      "Accuracy: 40.98%.\n",
      "Training Loss: 2.1406495571136475\n",
      "\n",
      "Epoch 1200\n",
      "Accuracy: 41.00%.\n",
      "Training Loss: 2.140623092651367\n",
      "\n",
      "Epoch 1300\n",
      "Accuracy: 41.02%.\n",
      "Training Loss: 2.1402645111083984\n",
      "\n",
      "Epoch 1400\n",
      "Accuracy: 41.06%.\n",
      "Training Loss: 2.13987135887146\n",
      "\n",
      "Epoch 1500\n",
      "Accuracy: 41.03%.\n",
      "Training Loss: 2.1395139694213867\n",
      "\n",
      "Epoch 1600\n",
      "Accuracy: 41.09%.\n",
      "Training Loss: 2.1393673419952393\n",
      "\n",
      "Epoch 1700\n",
      "Accuracy: 41.11%.\n",
      "Training Loss: 2.1391007900238037\n",
      "\n",
      "Epoch 1800\n",
      "Accuracy: 41.09%.\n",
      "Training Loss: 2.1392879486083984\n",
      "\n",
      "Epoch 1900\n",
      "Accuracy: 41.10%.\n",
      "Training Loss: 2.139003038406372\n",
      "\n",
      "Epoch 2000\n",
      "Accuracy: 41.07%.\n",
      "Training Loss: 2.1387739181518555\n",
      "\n",
      "Model completed in 54.52m.\n",
      "\n",
      "Starting training Model 6 / 8...\n",
      "Epoch 100\n",
      "Accuracy: 40.28%.\n",
      "Training Loss: 2.1720147132873535\n",
      "\n",
      "Epoch 200\n",
      "Accuracy: 40.51%.\n",
      "Training Loss: 2.166994571685791\n",
      "\n",
      "Epoch 300\n",
      "Accuracy: 40.63%.\n",
      "Training Loss: 2.1646387577056885\n",
      "\n",
      "Epoch 400\n",
      "Accuracy: 39.72%.\n",
      "Training Loss: 2.282529354095459\n",
      "\n",
      "Epoch 500\n",
      "Accuracy: 39.82%.\n",
      "Training Loss: 2.18048357963562\n",
      "\n",
      "Epoch 600\n",
      "Accuracy: 39.88%.\n",
      "Training Loss: 2.1762876510620117\n",
      "\n",
      "Epoch 700\n",
      "Accuracy: 40.10%.\n",
      "Training Loss: 2.1715142726898193\n",
      "\n",
      "Epoch 800\n",
      "Accuracy: 40.17%.\n",
      "Training Loss: 2.170245409011841\n",
      "\n",
      "Epoch 900\n",
      "Accuracy: 40.03%.\n",
      "Training Loss: 2.1716482639312744\n",
      "\n",
      "Epoch 1000\n",
      "Accuracy: 40.27%.\n",
      "Training Loss: 2.1687700748443604\n",
      "\n",
      "Epoch 1100\n",
      "Accuracy: 40.39%.\n",
      "Training Loss: 2.1671335697174072\n",
      "\n",
      "Epoch 1200\n",
      "Accuracy: 40.34%.\n",
      "Training Loss: 2.1668670177459717\n",
      "\n",
      "Epoch 1300\n",
      "Accuracy: 40.39%.\n",
      "Training Loss: 2.165715217590332\n",
      "\n",
      "Epoch 1400\n",
      "Accuracy: 40.45%.\n",
      "Training Loss: 2.165518045425415\n",
      "\n",
      "Epoch 1500\n",
      "Accuracy: 40.51%.\n",
      "Training Loss: 2.164609909057617\n",
      "\n",
      "Epoch 1600\n",
      "Accuracy: 40.39%.\n",
      "Training Loss: 2.165609836578369\n",
      "\n",
      "Epoch 1700\n",
      "Accuracy: 40.42%.\n",
      "Training Loss: 2.1660959720611572\n",
      "\n",
      "Epoch 1800\n",
      "Accuracy: 40.43%.\n",
      "Training Loss: 2.1655948162078857\n",
      "\n",
      "Epoch 1900\n",
      "Accuracy: 40.34%.\n",
      "Training Loss: 2.1667230129241943\n",
      "\n",
      "Epoch 2000\n",
      "Accuracy: 40.34%.\n",
      "Training Loss: 2.166722536087036\n",
      "\n",
      "Model completed in 54.78m.\n",
      "\n",
      "Starting training Model 7 / 8...\n",
      "Epoch 100\n",
      "Accuracy: 40.61%.\n",
      "Training Loss: 2.1532015800476074\n",
      "\n",
      "Epoch 200\n",
      "Accuracy: 40.69%.\n",
      "Training Loss: 2.150171995162964\n",
      "\n",
      "Epoch 300\n",
      "Accuracy: 40.82%.\n",
      "Training Loss: 2.1484405994415283\n",
      "\n",
      "Epoch 400\n",
      "Accuracy: 40.79%.\n",
      "Training Loss: 2.212583065032959\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500\n",
      "Accuracy: 40.89%.\n",
      "Training Loss: 2.1465303897857666\n",
      "\n",
      "Epoch 600\n",
      "Accuracy: 40.92%.\n",
      "Training Loss: 2.1460723876953125\n",
      "\n",
      "Epoch 700\n",
      "Accuracy: 40.94%.\n",
      "Training Loss: 2.146771192550659\n",
      "\n",
      "Epoch 800\n",
      "Accuracy: 40.91%.\n",
      "Training Loss: 2.145834445953369\n",
      "\n",
      "Epoch 900\n",
      "Accuracy: 40.90%.\n",
      "Training Loss: 2.1454970836639404\n",
      "\n",
      "Epoch 1000\n",
      "Accuracy: 40.88%.\n",
      "Training Loss: 2.145270586013794\n",
      "\n",
      "Epoch 1100\n",
      "Accuracy: 40.92%.\n",
      "Training Loss: 2.1451990604400635\n",
      "\n",
      "Epoch 1200\n",
      "Accuracy: 40.91%.\n",
      "Training Loss: 2.1450278759002686\n",
      "\n",
      "Epoch 1300\n",
      "Accuracy: 40.84%.\n",
      "Training Loss: 2.238878011703491\n",
      "\n",
      "Epoch 1400\n",
      "Accuracy: 40.94%.\n",
      "Training Loss: 2.1459786891937256\n",
      "\n",
      "Epoch 1500\n",
      "Accuracy: 40.87%.\n",
      "Training Loss: 2.148428201675415\n",
      "\n",
      "Epoch 1600\n",
      "Accuracy: 40.91%.\n",
      "Training Loss: 2.144831657409668\n",
      "\n",
      "Epoch 1700\n",
      "Accuracy: 40.91%.\n",
      "Training Loss: 2.144817352294922\n",
      "\n",
      "Epoch 1800\n",
      "Accuracy: 40.89%.\n",
      "Training Loss: 2.1448237895965576\n",
      "\n",
      "Epoch 1900\n",
      "Accuracy: 40.89%.\n",
      "Training Loss: 2.144845962524414\n",
      "\n",
      "Epoch 2000\n",
      "Accuracy: 40.88%.\n",
      "Training Loss: 2.1450068950653076\n",
      "\n",
      "Model completed in 121.96m.\n",
      "\n",
      "Starting training Model 8 / 8...\n",
      "Epoch 100\n",
      "Accuracy: 38.73%.\n",
      "Training Loss: 2.2324984073638916\n",
      "\n",
      "Epoch 200\n",
      "Accuracy: 38.65%.\n",
      "Training Loss: 2.772582530975342\n",
      "\n",
      "Epoch 300\n",
      "Accuracy: 38.52%.\n",
      "Training Loss: 2.2473270893096924\n",
      "\n",
      "Epoch 400\n",
      "Accuracy: 38.55%.\n",
      "Training Loss: 2.246995210647583\n",
      "\n",
      "Epoch 500\n",
      "Accuracy: 38.70%.\n",
      "Training Loss: 2.71759033203125\n",
      "\n",
      "Epoch 600\n",
      "Accuracy: 38.52%.\n",
      "Training Loss: 2.781041145324707\n",
      "\n",
      "Epoch 700\n",
      "Accuracy: 39.04%.\n",
      "Training Loss: 2.2234561443328857\n",
      "\n",
      "Epoch 800\n",
      "Accuracy: 39.05%.\n",
      "Training Loss: 2.2238521575927734\n",
      "\n",
      "Epoch 900\n",
      "Accuracy: 38.98%.\n",
      "Training Loss: 2.224277973175049\n",
      "\n",
      "Epoch 1000\n",
      "Accuracy: 38.89%.\n",
      "Training Loss: 2.2253897190093994\n",
      "\n",
      "Epoch 1100\n",
      "Accuracy: 38.75%.\n",
      "Training Loss: 2.231896162033081\n",
      "\n",
      "Epoch 1200\n",
      "Accuracy: 38.62%.\n",
      "Training Loss: 2.2379744052886963\n",
      "\n",
      "Epoch 1300\n",
      "Accuracy: 38.54%.\n",
      "Training Loss: 2.247164249420166\n",
      "\n",
      "Epoch 1400\n",
      "Accuracy: 38.53%.\n",
      "Training Loss: 2.717388391494751\n",
      "\n",
      "Epoch 1500\n",
      "Accuracy: 39.10%.\n",
      "Training Loss: 2.2239444255828857\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Non-Weighted Model Training.\n",
    "\n",
    "epochs = 2000\n",
    "\n",
    "models = []\n",
    "train_losses_list = []\n",
    "valid_losses = []\n",
    "\n",
    "for i, hyperparams in enumerate(hyperparams_list):\n",
    "    print(f'Starting training Model {i+1} / {len(hyperparams_list)}...')\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    batch_size = hyperparams['batch_size']\n",
    "    lr = hyperparams['lr']\n",
    "    train_weights, valid_weights = hyperparams['weights']\n",
    "\n",
    "    # 1. Setup Data Loader\n",
    "\n",
    "    dataset = DocDataset(train_sequence, train_offsets, train_labels, train_weights)\n",
    "\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False,\n",
    "                             collate_fn=collate_samples)\n",
    "\n",
    "    # 2. Create the Model\n",
    "\n",
    "    model = Model(embeddings=embeddings, n_classes=len(label_types))\n",
    "\n",
    "    # 3. Setup Criterion and Optimizer\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # 4. Train the Model\n",
    "\n",
    "    train_losses = train(model,\n",
    "                         criterion,\n",
    "                         optimizer,\n",
    "                         dataset,\n",
    "                         data_loader,\n",
    "                         epochs)\n",
    "    \n",
    "    # 5. Calculate Validation Loss\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_dataset = DocDataset(valid_sequence, valid_offsets, valid_labels, valid_weights)\n",
    "        valid_samples = valid_dataset.all_samples()\n",
    "\n",
    "        outputs = model(valid_samples['data'], valid_samples['offset'], valid_samples['weights'])\n",
    "\n",
    "        valid_loss = criterion(outputs, valid_labels)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "    end_time = time()\n",
    "\n",
    "    models.append(model)\n",
    "    train_losses_list.append(train_losses)\n",
    "\n",
    "    print(f'Model completed in {(end_time - start_time)/60:.02f}m.')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_mask = [hp['type'] == 'uniform' for hp in hyperparams_list]\n",
    "\n",
    "uniform_models = [m for i, m in enumerate(models) if uniform_mask[i]]\n",
    "uniform_train_losses_list = [losses for i, losses in enumerate(train_losses_list) if uniform_mask[i]]\n",
    "uniform_valid_losses = [loss.item() for i, loss in enumerate(valid_losses) if uniform_mask[i]]\n",
    "\n",
    "tf_idf_models = [m for i, m in enumerate(models) if not uniform_mask[i]]\n",
    "tf_idf_train_losses_list = [losses for i, losses in enumerate(train_losses_list) if not uniform_mask[i]]\n",
    "tf_idf_valid_losses = [loss.item() for i, loss in enumerate(valid_losses) if not uniform_mask[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss of uniform models.\n",
    "\n",
    "for i, model in enumerate(uniform_models):\n",
    "    train_losses = uniform_train_losses_list[i]\n",
    "    plt.plot(train_losses)\n",
    "\n",
    "plt.legend([f'Model {i+1}' for i in range(len(uniform_models))])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss of tf-idf models.\n",
    "\n",
    "for i, model in enumerate(tf_idf_models):\n",
    "    train_losses = tf_idf_train_losses_list[i]\n",
    "    plt.plot(train_losses)\n",
    "\n",
    "plt.legend([f'Model {i+1}' for i in range(len(tf_idf_models))])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Best Models\n",
    "\n",
    "We will grab the \"best\" model trained on uniform weights and tf-idf weights. This will be based on which model scores the lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the best models.\n",
    "\n",
    "best_uniform_model_idx = uniform_valid_losses.index(min(uniform_valid_losses))\n",
    "best_uniform_model = uniform_models[best_uniform_model_idx]\n",
    "\n",
    "best_tf_idf_model_idx = tf_idf_valid_losses.index(min(tf_idf_valid_losses))\n",
    "best_tf_idf_model = tf_idf_models[best_tf_idf_model_idx]\n",
    "\n",
    "print(f'Best Uniform Model: {best_uniform_model_idx+1}')\n",
    "print(f'Best TF-IDF Model:  {best_tf_idf_model_idx+1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_dataset = DocDataset(valid_sequence, valid_offsets, valid_labels, valid_uniform_weights)\n",
    "\n",
    "uniform_samples = uniform_dataset.all_samples()\n",
    "\n",
    "uniform_predictions = best_uniform_model.predict(uniform_samples['data'], uniform_samples['offset'], uniform_samples['weights'])\n",
    "\n",
    "total = len(uniform_samples['label'])\n",
    "correct = torch.sum(uniform_predictions == uniform_samples['label'])\n",
    "\n",
    "print(f'Accuracy of Uniform Model: {(float(correct) / total)*100:.02f}%.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dataset = DocDataset(valid_sequence, valid_offsets, valid_labels, valid_tf_idf_weights)\n",
    "\n",
    "tf_idf_samples = tf_idf_dataset.all_samples()\n",
    "\n",
    "tf_idf_predictions = best_tf_idf_model.predict(tf_idf_samples['data'], tf_idf_samples['offset'], tf_idf_samples['weights'])\n",
    "\n",
    "total = len(tf_idf_samples['label'])\n",
    "correct = torch.sum(tf_idf_predictions == tf_idf_samples['label'])\n",
    "\n",
    "print(f'Accuracy of TF-IDF Model: {(float(correct) / total)*100:.02f}%.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(labels, predictions):\n",
    "    # Displaying a confusion matrix of the validation results for our model.\n",
    "\n",
    "    categories = labels.unique()\n",
    "    category_encoder = { c.item():i for i,c in enumerate(categories) }\n",
    "\n",
    "    confusion_matrix = np.random.rand(len(categories), len(categories))\n",
    "\n",
    "    for i, category in enumerate(categories):\n",
    "        row = np.zeros(len(categories))\n",
    "\n",
    "        cat_mask = (labels == category.item()).tolist()\n",
    "        cat_preds = predictions[cat_mask]\n",
    "        \n",
    "        for category in categories:\n",
    "            pred_count = torch.sum(cat_preds == category.item())\n",
    "            row[category_encoder[category.item()]] = pred_count\n",
    "            \n",
    "        confusion_matrix[i, :] = row / len(cat_preds)\n",
    "\n",
    "    return confusion_matrix, category_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    cax = ax.matshow(confusion_matrix)\n",
    "\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Uniform Model\n",
    "\n",
    "uniform_confusion_matrix, category_encoder = create_confusion_matrix(valid_labels, uniform_predictions)\n",
    "show_confusion_matrix(uniform_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for TF-IDF Model\n",
    "\n",
    "tf_idf_confusion_matrix, category_encoder = create_confusion_matrix(valid_labels, tf_idf_predictions)\n",
    "show_confusion_matrix(tf_idf_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_uniform_model.state_dict(), './models/uniform_glove_model.torch')\n",
    "torch.save(best_tf_idf_model.state_dict(), './models/tf_idf_model.torch')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
