{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classifier\n",
    "\n",
    "**Note: There are some random processes within this notebook, so different runs of the notebook may result in different outcomes.**\n",
    "\n",
    "**Note: This notebook assumes the data being loaded has already been randomly shuffled.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from data_utils import BOWEncoding, WordEmbeddingEncoding, WordTokenDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Setup the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will circle back and add support for embedding layer.\n",
    "# embeddings = data_utils.load_embeddings('./data/glove.6B/glove.6B.100d.txt',\n",
    "#                                         embedding_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('./data/train_data.json', orient='records')\n",
    "data = data.sample(frac=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = 0.95\n",
    "split_idx = math.floor(len(data) * train_test_split)\n",
    "\n",
    "train_data = data.iloc[0:split_idx]\n",
    "valid_data = data.iloc[split_idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_encoding = BOWEncoding(data, min_word_freq=5)\n",
    "bow_encoding.prepare()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_train_dataset = WordTokenDataset(train_data, bow_encoding)\n",
    "bow_train_dataset.prepare()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_valid_dataset = WordTokenDataset(valid_data, bow_encoding)\n",
    "bow_valid_dataset.prepare()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow_batched_sequences(samples):\n",
    "    encoding_dim = samples.vocab_size + 1 # Add 1 for EOS token.\n",
    "    sequence_len = samples.longest_sequence + 1 # Add 1 for EOS token.\n",
    "    batch_size = len(samples.label)\n",
    "\n",
    "    data = torch.zeros(size=(sequence_len, batch_size, encoding_dim), dtype=torch.float)\n",
    "\n",
    "    # Looping through each token in each example. This is slow.\n",
    "    # TODO: Should find ways to make this faster. Vectorization? Caching?\n",
    "    for i, start_offset in enumerate(samples.offset):\n",
    "\n",
    "        end_offset = None if (i+1) >= len(samples.offset) else samples.offset[i+1]\n",
    "        sequence_slice = samples.sequence[start_offset:] if end_offset is None else samples.sequence[start_offset:end_offset]\n",
    "\n",
    "        for j, token_idx in enumerate(sequence_slice):\n",
    "            # jth token in ith example.\n",
    "            data[j, i, token_idx] = 1.\n",
    "\n",
    "        j += 1\n",
    "\n",
    "        while j < sequence_len:\n",
    "            # Set EOS one-hot encodings, padded at the end of each sequence.\n",
    "            data[j, i, encoding_dim - 1] = 1.\n",
    "            j += 1\n",
    "        \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_size = 3\n",
    "hid_size = 5\n",
    "out_size = 3\n",
    "lay_size = 2\n",
    "batch_size = 8\n",
    "\n",
    "lstm = nn.LSTM(inp_size, hid_size, lay_size)\n",
    "\n",
    "inp = torch.rand(size=(7, batch_size, inp_size))\n",
    "hid = (torch.rand(lay_size, batch_size, hid_size), torch.rand(lay_size, batch_size, hid_size))\n",
    "a, b = lstm(inp, hid)\n",
    "\n",
    "# a.size() # (seq_len x batch_size x hid_size)\n",
    "\n",
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, encoding, lstm_hidden_size, lstm_num_layers):\n",
    "        self.encoding = encoding\n",
    "        \n",
    "        input_size = encoding.n_inputs() + 1 # Add EOS tag to vocab.\n",
    "        output_size = encoding.n_classes()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, lstm_hidden_size, lstm_num_layers)\n",
    "        self.hidden2tag = nn.Linear(lstm_hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        _, hidden = self.lstm(input, hidden)\n",
    "        output = self.hidden2tag(hidden)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(size=(1, 1, encoding.n_classes()), dtype=torch.float)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
