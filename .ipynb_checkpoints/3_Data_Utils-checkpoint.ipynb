{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Utils\n",
    "\n",
    "The purpose of this notebook is to:\n",
    "\n",
    "1. Create utility operations for preparing and cleaning data that will be used to train the different models.\n",
    "2. Test out different utility methods to make sure they work as intended.\n",
    "3. Create a custom torch Dataset that can be plugged in to pytorch models.\n",
    "\n",
    "**NOTE: This code uses pseudo-random operations, so running this script will generate different data on each run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path, embedding_dim):\n",
    "    with open(path) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        index = []\n",
    "        embeddings = np.zeros((len(lines), embedding_dim))\n",
    "        \n",
    "        for i, l in enumerate(lines):\n",
    "            tokens = l.split(' ')\n",
    "            index.append(tokens[0])\n",
    "            embeddings[i, :] = tokens[1:]\n",
    "\n",
    "        return pd.DataFrame(embeddings, index=index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.038194</td>\n",
       "      <td>-0.244870</td>\n",
       "      <td>0.72812</td>\n",
       "      <td>-0.399610</td>\n",
       "      <td>0.083172</td>\n",
       "      <td>0.043953</td>\n",
       "      <td>-0.391410</td>\n",
       "      <td>0.334400</td>\n",
       "      <td>-0.57545</td>\n",
       "      <td>0.087459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016215</td>\n",
       "      <td>-0.017099</td>\n",
       "      <td>-0.389840</td>\n",
       "      <td>0.87424</td>\n",
       "      <td>-0.72569</td>\n",
       "      <td>-0.51058</td>\n",
       "      <td>-0.520280</td>\n",
       "      <td>-0.14590</td>\n",
       "      <td>0.82780</td>\n",
       "      <td>0.270620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.107670</td>\n",
       "      <td>0.110530</td>\n",
       "      <td>0.59812</td>\n",
       "      <td>-0.543610</td>\n",
       "      <td>0.673960</td>\n",
       "      <td>0.106630</td>\n",
       "      <td>0.038867</td>\n",
       "      <td>0.354810</td>\n",
       "      <td>0.06351</td>\n",
       "      <td>-0.094189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349510</td>\n",
       "      <td>-0.722600</td>\n",
       "      <td>0.375490</td>\n",
       "      <td>0.44410</td>\n",
       "      <td>-0.99059</td>\n",
       "      <td>0.61214</td>\n",
       "      <td>-0.351110</td>\n",
       "      <td>-0.83155</td>\n",
       "      <td>0.45293</td>\n",
       "      <td>0.082577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>-0.339790</td>\n",
       "      <td>0.209410</td>\n",
       "      <td>0.46348</td>\n",
       "      <td>-0.647920</td>\n",
       "      <td>-0.383770</td>\n",
       "      <td>0.038034</td>\n",
       "      <td>0.171270</td>\n",
       "      <td>0.159780</td>\n",
       "      <td>0.46619</td>\n",
       "      <td>-0.019169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063351</td>\n",
       "      <td>-0.674120</td>\n",
       "      <td>-0.068895</td>\n",
       "      <td>0.53604</td>\n",
       "      <td>-0.87773</td>\n",
       "      <td>0.31802</td>\n",
       "      <td>-0.392420</td>\n",
       "      <td>-0.23394</td>\n",
       "      <td>0.47298</td>\n",
       "      <td>-0.028803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-0.152900</td>\n",
       "      <td>-0.242790</td>\n",
       "      <td>0.89837</td>\n",
       "      <td>0.169960</td>\n",
       "      <td>0.535160</td>\n",
       "      <td>0.487840</td>\n",
       "      <td>-0.588260</td>\n",
       "      <td>-0.179820</td>\n",
       "      <td>-1.35810</td>\n",
       "      <td>0.425410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187120</td>\n",
       "      <td>-0.018488</td>\n",
       "      <td>-0.267570</td>\n",
       "      <td>0.72700</td>\n",
       "      <td>-0.59363</td>\n",
       "      <td>-0.34839</td>\n",
       "      <td>-0.560940</td>\n",
       "      <td>-0.59100</td>\n",
       "      <td>1.00390</td>\n",
       "      <td>0.206640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-0.189700</td>\n",
       "      <td>0.050024</td>\n",
       "      <td>0.19084</td>\n",
       "      <td>-0.049184</td>\n",
       "      <td>-0.089737</td>\n",
       "      <td>0.210060</td>\n",
       "      <td>-0.549520</td>\n",
       "      <td>0.098377</td>\n",
       "      <td>-0.20135</td>\n",
       "      <td>0.342410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131340</td>\n",
       "      <td>0.058617</td>\n",
       "      <td>-0.318690</td>\n",
       "      <td>-0.61419</td>\n",
       "      <td>-0.62393</td>\n",
       "      <td>-0.41548</td>\n",
       "      <td>-0.038175</td>\n",
       "      <td>-0.39804</td>\n",
       "      <td>0.47647</td>\n",
       "      <td>-0.159830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1        2         3         4         5         6   \\\n",
       "the -0.038194 -0.244870  0.72812 -0.399610  0.083172  0.043953 -0.391410   \n",
       ",   -0.107670  0.110530  0.59812 -0.543610  0.673960  0.106630  0.038867   \n",
       ".   -0.339790  0.209410  0.46348 -0.647920 -0.383770  0.038034  0.171270   \n",
       "of  -0.152900 -0.242790  0.89837  0.169960  0.535160  0.487840 -0.588260   \n",
       "to  -0.189700  0.050024  0.19084 -0.049184 -0.089737  0.210060 -0.549520   \n",
       "\n",
       "           7        8         9   ...        90        91        92       93  \\\n",
       "the  0.334400 -0.57545  0.087459  ...  0.016215 -0.017099 -0.389840  0.87424   \n",
       ",    0.354810  0.06351 -0.094189  ...  0.349510 -0.722600  0.375490  0.44410   \n",
       ".    0.159780  0.46619 -0.019169  ... -0.063351 -0.674120 -0.068895  0.53604   \n",
       "of  -0.179820 -1.35810  0.425410  ...  0.187120 -0.018488 -0.267570  0.72700   \n",
       "to   0.098377 -0.20135  0.342410  ... -0.131340  0.058617 -0.318690 -0.61419   \n",
       "\n",
       "          94       95        96       97       98        99  \n",
       "the -0.72569 -0.51058 -0.520280 -0.14590  0.82780  0.270620  \n",
       ",   -0.99059  0.61214 -0.351110 -0.83155  0.45293  0.082577  \n",
       ".   -0.87773  0.31802 -0.392420 -0.23394  0.47298 -0.028803  \n",
       "of  -0.59363 -0.34839 -0.560940 -0.59100  1.00390  0.206640  \n",
       "to  -0.62393 -0.41548 -0.038175 -0.39804  0.47647 -0.159830  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = load_embeddings('./data/glove.6B/glove.6B.100d.txt', embedding_dim=100)\n",
    "\n",
    "embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data into dataframe.\n",
    "data = pd.read_json('./data/train_data.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(df, c):\n",
    "    category_counts = df['category'].value_counts()\n",
    "    min_count = category_counts.min()\n",
    "\n",
    "    # Calculate the probability of keeping a row\n",
    "    # of a given category.\n",
    "    category_probs = (min_count / category_counts) ** (1/c)\n",
    "\n",
    "    # This is a series used to determine the probability that each\n",
    "    # row is kept. Each rows mask depends on its category.\n",
    "    prob_mask = np.zeros(len(df))\n",
    "\n",
    "    for i, category in enumerate(category_counts.index.tolist()):\n",
    "        category_prob = category_probs[i]\n",
    "        category_keep_mask = (df['category'] == category) * category_prob\n",
    "        prob_mask = prob_mask + category_keep_mask\n",
    "\n",
    "    keep_mask = np.random.rand(len(df)) <= prob_mask\n",
    "    \n",
    "    return df[keep_mask].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.tokenize.regexp import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special token for tokens that occur MIN_WORD_FREQ or fewer times in the\n",
    "# entire corpus.\n",
    "__LOW_FREQ_TOKEN__ = '__LOW_FREQ_TOKEN__'\n",
    "\n",
    "MIN_WORD_FREQ = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_and_tokenize_text(text):\n",
    "    cleaned = ''.join([c for c in text if c not in string.punctuation]).lower()\n",
    "    return tokenizer.tokenize(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_rows(df):\n",
    "    tokenized_headlines = df['headline'].apply(cleanup_and_tokenize_text).tolist()\n",
    "    tokenized_desc = df['short_description'].apply(cleanup_and_tokenize_text).tolist()\n",
    "\n",
    "    return [tokens1 + tokens2 for tokens1, tokens2 in zip(tokenized_headlines, tokenized_desc)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unigram_counts(rows):\n",
    "    # Flatten\n",
    "    tokens = [t for tokens in rows for t in tokens]\n",
    "    \n",
    "    counts = {}\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in counts:\n",
    "            counts[token] = 0\n",
    "        counts[token] += 1\n",
    "\n",
    "    return counts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordTokenDatasetSample():\n",
    "    def __init__(self, sequence, offset, label, vocab_size):\n",
    "        self.sequence = sequence\n",
    "        self.offset = offset\n",
    "        self.label = label\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordTokenDataset(Dataset):\n",
    "    __TOKEN_UNK__ = '__TOKEN_UNK__'\n",
    "\n",
    "    __TOKEN_LOW_FREQ__ = '__TOKEN_LOW_FREQ__'\n",
    "\n",
    "    def __init__(self, data, downsample_c=None, accepted_tokens=None, min_word_freq=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.downsample_c = downsample_c\n",
    "        self.accepted_tokens = accepted_tokens\n",
    "        self.min_word_freq = min_word_freq\n",
    "\n",
    "        self._is_prepared = False\n",
    "        self._unigram_counts = None\n",
    "        self._token_encoder = None\n",
    "        self._encoded_to_idx = None\n",
    "        self._label_encoder = None\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if type(idx) == int:\n",
    "            idx = slice(idx, idx+1)\n",
    "\n",
    "        assert(self._is_prepared)\n",
    "\n",
    "        sub_data = self.data.iloc[idx]\n",
    "        \n",
    "        if len(sub_data) == 0:\n",
    "            return WordTokenDatasetSample(sequence=torch.LongTensor([]),\n",
    "                                          offset=torch.LongTensor([]),\n",
    "                                          label=torch.LongTensor([]),\n",
    "                                          vocab_size=len(self._encoded_to_idx))\n",
    "\n",
    "        tokenized_rows = tokenize_rows(sub_data)\n",
    "\n",
    "        offset = []\n",
    "        sequence = []\n",
    "        \n",
    "        for i, tokens in enumerate(tokenized_rows):\n",
    "            sub_sequence = [self._encoded_to_idx[self._token_encoder[t]] for t in tokens]\n",
    "            sequence.extend(sub_sequence)\n",
    "            offset.append(len(sequence) - len(sub_sequence))\n",
    "        \n",
    "        label = [self._label_encoder[l] for l in sub_data['category']]\n",
    "\n",
    "        return WordTokenDatasetSample(sequence=torch.LongTensor(sequence),\n",
    "                                      offset=torch.LongTensor(offset),\n",
    "                                      label=torch.LongTensor(label),\n",
    "                                      vocab_size=len(self._encoded_to_idx))\n",
    "\n",
    "\n",
    "    def prepare(self):\n",
    "        if self.downsample_c is not None:\n",
    "            self.data = downsample(self.data, self.downsample_c)\n",
    "\n",
    "        tokenized_rows = tokenize_rows(self.data)\n",
    "\n",
    "        self._unigram_counts = create_unigram_counts(tokenized_rows)\n",
    "        self._token_encoder = { t : self._encoded_token(t) for t in self._unigram_counts }\n",
    "        self._encoded_to_idx = { t:i for i,t in enumerate(self._token_encoder.values()) }\n",
    "        self._label_encoder = {l:i for i,l in enumerate(self.data['category'].unique()) }\n",
    "        \n",
    "        # Remove any rows in data that have no tokens.\n",
    "        keep_mask = np.zeros(len(tokenized_rows))\n",
    "        for i, ts in enumerate(tokenized_rows):\n",
    "            # This will be true if there exists a token that is encoded into itself.\n",
    "            # (i.e. not an unknown token or low freq token).\n",
    "            keep_mask[i] = len([True for t in ts if self._encoded_token(t) == t]) > 0\n",
    "\n",
    "        keep_mask = keep_mask.astype(bool)\n",
    "        self.data = self.data.iloc[keep_mask]\n",
    "        self._is_prepared = True\n",
    "\n",
    "\n",
    "    def _encoded_token(self, token):\n",
    "        assert(self._unigram_counts is not None)\n",
    "\n",
    "        if self.accepted_tokens is not None and token not in self.accepted_tokens:\n",
    "            return self.__TOKEN_UNK__\n",
    "        elif token not in self._unigram_counts:\n",
    "            return self.__TOKEN_UNK__\n",
    "        elif self._unigram_counts[token] < self.min_word_freq:\n",
    "            return self.__TOKEN_LOW_FREQ__\n",
    "        return token\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_samples(samples):\n",
    "    if len(samples) == 0:\n",
    "        return WordTokenDatasetSample(sequence=torch.LongTensor([]),\n",
    "                                      offset=torch.LongTensor([]),\n",
    "                                      label=torch.LongTensor([]),\n",
    "                                      vocab_size=0)\n",
    "\n",
    "    label = torch.cat([s.label for s in samples])\n",
    "    sequence = torch.cat([s.sequence for s in samples])\n",
    "    vocab_size = samples[0].vocab_size\n",
    "\n",
    "    offset = torch.zeros_like(label, dtype=torch.int64)\n",
    "    iter = 0\n",
    "    shift_val = 0\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        print(iter)\n",
    "        sample_offset = sample.offset\n",
    "        offset[iter:(iter+len(sample_offset))] = (sample_offset + shift_val)\n",
    "\n",
    "        iter = iter + len(sample_offset)\n",
    "        shift_val = shift_val + len(samples[i].sequence)\n",
    "    \n",
    "    return WordTokenDatasetSample(sequence=sequence,\n",
    "                                  offset=offset,\n",
    "                                  label=label,\n",
    "                                  vocab_size=vocab_size)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
