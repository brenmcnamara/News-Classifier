{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Utils\n",
    "\n",
    "The purpose of this notebook is to:\n",
    "\n",
    "1. Create utility operations for preparing and cleaning data that will be used to train the different models.\n",
    "2. Test out different utility methods to make sure they work as intended.\n",
    "3. Create a custom torch Dataset that can be plugged in to pytorch models.\n",
    "\n",
    "**NOTE: This code uses pseudo-random operations, so running this script will generate different data on each run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path, embedding_dim):\n",
    "    with open(path) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        index = []\n",
    "        embeddings = np.zeros((len(lines), embedding_dim))\n",
    "        \n",
    "        for i, l in enumerate(lines):\n",
    "            tokens = l.split(' ')\n",
    "            index.append(tokens[0])\n",
    "            embeddings[i, :] = tokens[1:]\n",
    "\n",
    "        return pd.DataFrame(embeddings, index=index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_embeddings('./data/glove.6B/glove.6B.100d.txt', embedding_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data into dataframe.\n",
    "data = pd.read_json('./data/train_data.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(df, c):\n",
    "    category_counts = df['category'].value_counts()\n",
    "    min_count = category_counts.min()\n",
    "\n",
    "    # Calculate the probability of keeping a row\n",
    "    # of a given category.\n",
    "    category_probs = (min_count / category_counts) ** (1/c)\n",
    "\n",
    "    # This is a series used to determine the probability that each\n",
    "    # row is kept. Each rows mask depends on its category.\n",
    "    prob_mask = np.zeros(len(df))\n",
    "\n",
    "    for i, category in enumerate(category_counts.index.tolist()):\n",
    "        category_prob = category_probs[i]\n",
    "        category_keep_mask = (df['category'] == category) * category_prob\n",
    "        prob_mask = prob_mask + category_keep_mask\n",
    "\n",
    "    keep_mask = np.random.rand(len(df)) <= prob_mask\n",
    "    \n",
    "    return df[keep_mask].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.tokenize.regexp import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_and_tokenize_text(text):\n",
    "    cleaned = ''.join([c for c in text if c not in string.punctuation]).lower()\n",
    "    return tokenizer.tokenize(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_rows(df):\n",
    "    tokenized_headlines = df['headline'].apply(cleanup_and_tokenize_text).tolist()\n",
    "    tokenized_desc = df['short_description'].apply(cleanup_and_tokenize_text).tolist()\n",
    "\n",
    "    return [tokens1 + tokens2 for tokens1, tokens2 in zip(tokenized_headlines, tokenized_desc)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unigram_counts(rows):\n",
    "    # Flatten\n",
    "    tokens = [t for tokens in rows for t in tokens]\n",
    "    \n",
    "    counts = {}\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in counts:\n",
    "            counts[token] = 0\n",
    "        counts[token] += 1\n",
    "\n",
    "    return counts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWEncoding():\n",
    "    LOW_FREQ_TOKEN = '__LOW_FREQ_TOKEN__'\n",
    "\n",
    "    def __init__(self, data, min_word_freq=0):\n",
    "        self.data = data\n",
    "        self.min_word_freq = min_word_freq\n",
    "        \n",
    "        self._is_prepared = False\n",
    "        self.vocab_size = None\n",
    "        self._label_encoder = None\n",
    "        self._label_decoder = None\n",
    "        self._token_encoder = None\n",
    "        self._token_decoder = None\n",
    "\n",
    "    def prepare(self):\n",
    "        tokenized_rows = tokenize_rows(self.data)\n",
    "        unigram_counts = create_unigram_counts(tokenized_rows)\n",
    "        \n",
    "        if self.min_word_freq > 0:\n",
    "            tokenized_rows = [[t if unigram_counts[t] >= self.min_word_freq else self.LOW_FREQ_TOKEN for t in tokens] for tokens in tokenized_rows]\n",
    "            unigram_counts = create_unigram_counts(tokenized_rows)\n",
    "            \n",
    "        self._unigram_counts = unigram_counts\n",
    "        self._label_encoder = { l:i for i,l in enumerate(self.data['category'].unique()) }\n",
    "        self._label_decoder = { i:l for l,i in self._label_encoder.items() }\n",
    "        self._token_encoder = { t:i for i,t in enumerate(unigram_counts.keys()) }\n",
    "        self._token_decoder = { i:t for t,i in self._token_encoder.items() }\n",
    "\n",
    "        self.vocab_size = len(unigram_counts)\n",
    "\n",
    "        self._is_prepared = True\n",
    "\n",
    "    def encode_token(self, token):\n",
    "        assert(self._is_prepared)\n",
    "        return self._token_encoder[token]\n",
    "    \n",
    "    def encode_label(self, label):\n",
    "        assert(self._is_prepared)\n",
    "        return self._label_encoder[label]\n",
    "\n",
    "    def decode_label(self, label_idx):\n",
    "        assert(self._is_prepared)\n",
    "        return self._label_decoder[label_idx]\n",
    "\n",
    "    def n_classes(self):\n",
    "        assert(self._is_prepared)\n",
    "        return len(self._label_encoder)\n",
    "    \n",
    "    def is_valid_token(self, token):\n",
    "        assert(self._is_prepared)\n",
    "        return token in self._token_encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingEncoding():\n",
    "    def __init__(self, data, embeddings, min_word_freq=0):\n",
    "        self.data = data\n",
    "        self.embeddings = embeddings\n",
    "        self.min_word_freq = min_word_freq\n",
    "        self.vocab_size = len(embeddings)\n",
    "\n",
    "        self._is_prepared = False\n",
    "        self._embedding_tokens = None\n",
    "        self._unigram_counts = None\n",
    "        self._token_encoder = None\n",
    "        self._label_encoder = None\n",
    "\n",
    "\n",
    "    def prepare(self):\n",
    "        self._embedding_tokens = { t for t in self.embeddings.index }\n",
    "\n",
    "        tokenized_rows = tokenize_rows(self.data)\n",
    "        self._unigram_counts = create_unigram_counts(tokenized_rows)\n",
    "\n",
    "        # Filter out any tokens that are invalid.\n",
    "        tokenized_rows = [[t for t in tokens if self.is_valid_token(t)] for tokens in tokenized_rows] \n",
    "        \n",
    "        # Need to re-generate the unigram counts after\n",
    "        # performing this filtering.\n",
    "        self._unigram_counts = create_unigram_counts(tokenized_rows)\n",
    "\n",
    "        self._token_encoder = { t:i for i,t in enumerate(self.embeddings.index) }\n",
    "        self._label_encoder = { l:i for i,l in enumerate(self.data['category'].unique()) }\n",
    "\n",
    "        self._is_prepared = True\n",
    "\n",
    "    def encode_token(self, token):\n",
    "        assert(self._is_prepared)\n",
    "        return self._token_encoder[token]\n",
    "    \n",
    "    def encode_label(self, label):\n",
    "        assert(self._is_prepared)\n",
    "        return self._label_encoder[label]\n",
    "        \n",
    "    def n_classes(self):\n",
    "        assert(self._is_prepared)\n",
    "        return len(self._label_encoder)\n",
    "    \n",
    "\n",
    "    def is_valid_token(self, token):\n",
    "        assert(self._embedding_tokens is not None)\n",
    "        assert(self._unigram_counts is not None)\n",
    "\n",
    "        if token not in self._embedding_tokens:\n",
    "            return False\n",
    "        elif token not in self._unigram_counts or self._unigram_counts[token] < self.min_word_freq:\n",
    "            return False\n",
    "        return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordTokenDatasetSample():\n",
    "    def __init__(self, sequence, offset, label, vocab_size, doc_count, doc_freq):\n",
    "        self.sequence = sequence\n",
    "        self.offset = offset\n",
    "        self.label = label\n",
    "        self.vocab_size = vocab_size\n",
    "        self.doc_count = doc_count\n",
    "        self.doc_freq = doc_freq\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "\n",
    "    def create_bow_matrix(self):\n",
    "        bow = torch.zeros(size=(len(self.offset), self.vocab_size), dtype=torch.int64)\n",
    "        offset_with_end = torch.cat([self.offset, torch.LongTensor([len(self.sequence)])])\n",
    "        \n",
    "        for i in range(len(offset_with_end) - 1):\n",
    "            start = offset_with_end[i].item()\n",
    "            end = offset_with_end[i+1]\n",
    "            sub_seq = self.sequence[start:end]\n",
    "            for idx in sub_seq:\n",
    "                bow[i, idx.item()] += 1\n",
    "                \n",
    "        return bow\n",
    "\n",
    "\n",
    "    def create_uniform_weights(self):\n",
    "        if len(self) == 0:\n",
    "            return torch.FloatTensor([])\n",
    "        \n",
    "        weights = torch.zeros_like(self.sequence, dtype=torch.float)\n",
    "\n",
    "        offset_with_end = torch.cat([self.offset, torch.LongTensor([len(self.sequence)])])\n",
    "\n",
    "        for i in range(len(offset_with_end) - 1):\n",
    "            start = offset_with_end[i].item()\n",
    "            end = offset_with_end[i+1].item()\n",
    "            weight = 1. / (end - start)\n",
    "            \n",
    "            weights[start:end] = weight\n",
    "            \n",
    "        return weights\n",
    "\n",
    "\n",
    "    def create_tf_idf_weights(self):\n",
    "        if len(self) == 0:\n",
    "            return torch.FloatTensor([])\n",
    "        \n",
    "        weights = torch.zeros_like(self.sequence, dtype=torch.float)\n",
    "\n",
    "        offset_with_end = torch.cat([self.offset, torch.LongTensor([len(self.sequence)])])\n",
    "        idf = torch.log(torch.FloatTensor([float(self.doc_count) / self.doc_freq[idx.item()] for idx in self.sequence]))\n",
    "\n",
    "        for i in range(len(offset_with_end) - 1):\n",
    "            start = offset_with_end[i].item()\n",
    "            end = offset_with_end[i+1].item()\n",
    "            doc_len = end - start\n",
    "            \n",
    "            # Generate term frequencies for each element in sequence.\n",
    "            freq_map = {}\n",
    "            for idx in self.sequence[start:end]:\n",
    "                if idx not in freq_map:\n",
    "                    freq_map[idx.item()] = 1\n",
    "                else:\n",
    "                    freq_map[idx.item()] +=1\n",
    "            \n",
    "            tf = torch.FloatTensor([float(freq_map[idx.item()]) / doc_len for idx in self.sequence[start:end]])\n",
    "            tf_idf = tf * idf[start:end]\n",
    "            \n",
    "            # normalize tf_idf weights between documents to account for\n",
    "            # documents of very different sizes.\n",
    "            total = torch.sum(tf_idf)\n",
    "\n",
    "            weights[start:end] = tf_idf / total\n",
    "            \n",
    "        return weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordTokenDataset(Dataset):\n",
    "    def __init__(self, data, encoding):\n",
    "        \"\"\"\n",
    "        data: A pandas data frame where each row is a news article.\n",
    "\n",
    "        encoding: The word token encoding that contains information\n",
    "                  about the corpus and the encoding of words\n",
    "                  and labels.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.encoding = encoding\n",
    "\n",
    "        self._is_prepared = False\n",
    "        self._doc_freq = None\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if type(idx) == int:\n",
    "            idx = slice(idx, idx+1)\n",
    "\n",
    "        assert(self._is_prepared)\n",
    "\n",
    "        sub_data = self.data.iloc[idx]\n",
    "        \n",
    "        if len(sub_data) == 0:\n",
    "            return WordTokenDatasetSample(sequence=torch.LongTensor([]),\n",
    "                                          offset=torch.LongTensor([]),\n",
    "                                          label=torch.LongTensor([]),\n",
    "                                          vocab_size=self.encoding.vocab_size,\n",
    "                                          doc_count=len(self.data),\n",
    "                                          doc_freq=self._doc_freq)\n",
    "\n",
    "        tokenized_rows = tokenize_rows(sub_data)\n",
    "\n",
    "        offset = []\n",
    "        sequence = []\n",
    "        \n",
    "        for i, tokens in enumerate(tokenized_rows):\n",
    "            sub_sequence = [self.encoding.encode_token(t) for t in tokens if self.encoding.is_valid_token(t)]\n",
    "            sequence.extend(sub_sequence)\n",
    "            offset.append(len(sequence) - len(sub_sequence))\n",
    "        \n",
    "        label = [self.encoding.encode_label(l) for l in sub_data['category']]\n",
    "\n",
    "        return WordTokenDatasetSample(sequence=torch.LongTensor(sequence),\n",
    "                                      offset=torch.LongTensor(offset),\n",
    "                                      label=torch.LongTensor(label),\n",
    "                                      vocab_size=self.encoding.vocab_size,\n",
    "                                      doc_count=len(self.data),\n",
    "                                      doc_freq=self._doc_freq)\n",
    "\n",
    "\n",
    "    def prepare(self):\n",
    "        tokenized_rows = tokenize_rows(self.data)\n",
    "        \n",
    "        doc_freq = {}\n",
    "        for i, tokens in enumerate(tokenized_rows):\n",
    "            new_tokens = [t for t in tokens if self.encoding.is_valid_token(t)]\n",
    "            tokenized_rows[i] = new_tokens\n",
    "            \n",
    "            seen = set()\n",
    "            for token in new_tokens:\n",
    "                if token in seen:\n",
    "                    continue\n",
    "\n",
    "                idx = self.encoding.encode_token(token)\n",
    "                if idx not in doc_freq:\n",
    "                    doc_freq[idx] = 1\n",
    "                else:\n",
    "                    doc_freq[idx] += 1\n",
    "\n",
    "                seen.add(token)\n",
    "\n",
    "        # Remove any token rows that are empty.\n",
    "        keep_mask = np.array([len(ts) > 0 for ts in tokenized_rows])\n",
    "\n",
    "        self.data = self.data.iloc[keep_mask]\n",
    "        self._doc_freq = doc_freq\n",
    "        self._is_prepared = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_samples(samples):\n",
    "    if len(samples) == 0:\n",
    "        return WordTokenDatasetSample(sequence=torch.LongTensor([]),\n",
    "                                      offset=torch.LongTensor([]),\n",
    "                                      label=torch.LongTensor([]),\n",
    "                                      vocab_size=0,\n",
    "                                      doc_freq={},\n",
    "                                      doc_count=0)\n",
    "\n",
    "    label = torch.cat([s.label for s in samples])\n",
    "    sequence = torch.cat([s.sequence for s in samples])\n",
    "    vocab_size = samples[0].vocab_size\n",
    "    doc_freq = samples[0].doc_freq\n",
    "    doc_count = samples[0].doc_count\n",
    "\n",
    "    offset = torch.zeros_like(label, dtype=torch.int64)\n",
    "    iter = 0\n",
    "    shift_val = 0\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        sample_offset = sample.offset\n",
    "        offset[iter:(iter+len(sample_offset))] = (sample_offset + shift_val)\n",
    "\n",
    "        iter = iter + len(sample_offset)\n",
    "        shift_val = shift_val + len(samples[i].sequence)\n",
    "    \n",
    "    return WordTokenDatasetSample(sequence=sequence,\n",
    "                                  offset=offset,\n",
    "                                  label=label,\n",
    "                                  vocab_size=vocab_size,\n",
    "                                  doc_freq=doc_freq,\n",
    "                                  doc_count=doc_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_encoding = WordEmbeddingEncoding(data, embeddings)\n",
    "emb_encoding.prepare()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_encoding = BOWEncoding(data, min_word_freq=5)\n",
    "bow_encoding.prepare()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordTokenDataset(data, bow_encoding)\n",
    "dataset.prepare()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset,\n",
    "                         batch_size=100,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=collate_samples)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
