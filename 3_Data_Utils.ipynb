{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Utils\n",
    "\n",
    "The purpose of this notebook is to:\n",
    "\n",
    "1. Create utility operations for preparing and cleaning data that will be used to train the different models.\n",
    "2. Test out different utility methods to make sure they work as intended.\n",
    "3. Create a custom torch Dataset that can be plugged in to pytorch models.\n",
    "\n",
    "**NOTE: This code uses pseudo-random operations, so running this script will generate different data on each run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path, embedding_dim):\n",
    "    with open(path) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        index = []\n",
    "        embeddings = np.zeros((len(lines), embedding_dim))\n",
    "        \n",
    "        for i, l in enumerate(lines):\n",
    "            tokens = l.split(' ')\n",
    "            index.append(tokens[0])\n",
    "            embeddings[i, :] = tokens[1:]\n",
    "\n",
    "        return pd.DataFrame(embeddings, index=index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.038194</td>\n",
       "      <td>-0.244870</td>\n",
       "      <td>0.72812</td>\n",
       "      <td>-0.399610</td>\n",
       "      <td>0.083172</td>\n",
       "      <td>0.043953</td>\n",
       "      <td>-0.391410</td>\n",
       "      <td>0.334400</td>\n",
       "      <td>-0.57545</td>\n",
       "      <td>0.087459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016215</td>\n",
       "      <td>-0.017099</td>\n",
       "      <td>-0.389840</td>\n",
       "      <td>0.87424</td>\n",
       "      <td>-0.72569</td>\n",
       "      <td>-0.51058</td>\n",
       "      <td>-0.520280</td>\n",
       "      <td>-0.14590</td>\n",
       "      <td>0.82780</td>\n",
       "      <td>0.270620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.107670</td>\n",
       "      <td>0.110530</td>\n",
       "      <td>0.59812</td>\n",
       "      <td>-0.543610</td>\n",
       "      <td>0.673960</td>\n",
       "      <td>0.106630</td>\n",
       "      <td>0.038867</td>\n",
       "      <td>0.354810</td>\n",
       "      <td>0.06351</td>\n",
       "      <td>-0.094189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349510</td>\n",
       "      <td>-0.722600</td>\n",
       "      <td>0.375490</td>\n",
       "      <td>0.44410</td>\n",
       "      <td>-0.99059</td>\n",
       "      <td>0.61214</td>\n",
       "      <td>-0.351110</td>\n",
       "      <td>-0.83155</td>\n",
       "      <td>0.45293</td>\n",
       "      <td>0.082577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>-0.339790</td>\n",
       "      <td>0.209410</td>\n",
       "      <td>0.46348</td>\n",
       "      <td>-0.647920</td>\n",
       "      <td>-0.383770</td>\n",
       "      <td>0.038034</td>\n",
       "      <td>0.171270</td>\n",
       "      <td>0.159780</td>\n",
       "      <td>0.46619</td>\n",
       "      <td>-0.019169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063351</td>\n",
       "      <td>-0.674120</td>\n",
       "      <td>-0.068895</td>\n",
       "      <td>0.53604</td>\n",
       "      <td>-0.87773</td>\n",
       "      <td>0.31802</td>\n",
       "      <td>-0.392420</td>\n",
       "      <td>-0.23394</td>\n",
       "      <td>0.47298</td>\n",
       "      <td>-0.028803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-0.152900</td>\n",
       "      <td>-0.242790</td>\n",
       "      <td>0.89837</td>\n",
       "      <td>0.169960</td>\n",
       "      <td>0.535160</td>\n",
       "      <td>0.487840</td>\n",
       "      <td>-0.588260</td>\n",
       "      <td>-0.179820</td>\n",
       "      <td>-1.35810</td>\n",
       "      <td>0.425410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187120</td>\n",
       "      <td>-0.018488</td>\n",
       "      <td>-0.267570</td>\n",
       "      <td>0.72700</td>\n",
       "      <td>-0.59363</td>\n",
       "      <td>-0.34839</td>\n",
       "      <td>-0.560940</td>\n",
       "      <td>-0.59100</td>\n",
       "      <td>1.00390</td>\n",
       "      <td>0.206640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-0.189700</td>\n",
       "      <td>0.050024</td>\n",
       "      <td>0.19084</td>\n",
       "      <td>-0.049184</td>\n",
       "      <td>-0.089737</td>\n",
       "      <td>0.210060</td>\n",
       "      <td>-0.549520</td>\n",
       "      <td>0.098377</td>\n",
       "      <td>-0.20135</td>\n",
       "      <td>0.342410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131340</td>\n",
       "      <td>0.058617</td>\n",
       "      <td>-0.318690</td>\n",
       "      <td>-0.61419</td>\n",
       "      <td>-0.62393</td>\n",
       "      <td>-0.41548</td>\n",
       "      <td>-0.038175</td>\n",
       "      <td>-0.39804</td>\n",
       "      <td>0.47647</td>\n",
       "      <td>-0.159830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1        2         3         4         5         6   \\\n",
       "the -0.038194 -0.244870  0.72812 -0.399610  0.083172  0.043953 -0.391410   \n",
       ",   -0.107670  0.110530  0.59812 -0.543610  0.673960  0.106630  0.038867   \n",
       ".   -0.339790  0.209410  0.46348 -0.647920 -0.383770  0.038034  0.171270   \n",
       "of  -0.152900 -0.242790  0.89837  0.169960  0.535160  0.487840 -0.588260   \n",
       "to  -0.189700  0.050024  0.19084 -0.049184 -0.089737  0.210060 -0.549520   \n",
       "\n",
       "           7        8         9   ...        90        91        92       93  \\\n",
       "the  0.334400 -0.57545  0.087459  ...  0.016215 -0.017099 -0.389840  0.87424   \n",
       ",    0.354810  0.06351 -0.094189  ...  0.349510 -0.722600  0.375490  0.44410   \n",
       ".    0.159780  0.46619 -0.019169  ... -0.063351 -0.674120 -0.068895  0.53604   \n",
       "of  -0.179820 -1.35810  0.425410  ...  0.187120 -0.018488 -0.267570  0.72700   \n",
       "to   0.098377 -0.20135  0.342410  ... -0.131340  0.058617 -0.318690 -0.61419   \n",
       "\n",
       "          94       95        96       97       98        99  \n",
       "the -0.72569 -0.51058 -0.520280 -0.14590  0.82780  0.270620  \n",
       ",   -0.99059  0.61214 -0.351110 -0.83155  0.45293  0.082577  \n",
       ".   -0.87773  0.31802 -0.392420 -0.23394  0.47298 -0.028803  \n",
       "of  -0.59363 -0.34839 -0.560940 -0.59100  1.00390  0.206640  \n",
       "to  -0.62393 -0.41548 -0.038175 -0.39804  0.47647 -0.159830  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = load_embeddings('./data/glove.6B/glove.6B.100d.txt', embedding_dim=100)\n",
    "\n",
    "embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data into dataframe.\n",
    "data = pd.read_json('./data/train_data.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(df, c):\n",
    "    category_counts = df['category'].value_counts()\n",
    "    min_count = category_counts.min()\n",
    "\n",
    "    # Calculate the probability of keeping a row\n",
    "    # of a given category.\n",
    "    category_probs = (min_count / category_counts) ** (1/c)\n",
    "\n",
    "    # This is a series used to determine the probability that each\n",
    "    # row is kept. Each rows mask depends on its category.\n",
    "    prob_mask = np.zeros(len(df))\n",
    "\n",
    "    for i, category in enumerate(category_counts.index.tolist()):\n",
    "        category_prob = category_probs[i]\n",
    "        category_keep_mask = (df['category'] == category) * category_prob\n",
    "        prob_mask = prob_mask + category_keep_mask\n",
    "\n",
    "    keep_mask = np.random.rand(len(df)) <= prob_mask\n",
    "    \n",
    "    return df[keep_mask].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.tokenize.regexp import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special token for tokens that occur MIN_WORD_FREQ or fewer times in the\n",
    "# entire corpus.\n",
    "__LOW_FREQ_TOKEN__ = '__LOW_FREQ_TOKEN__'\n",
    "\n",
    "MIN_WORD_FREQ = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_and_tokenize_text(text):\n",
    "    cleaned = ''.join([c for c in text if c not in string.punctuation]).lower()\n",
    "    return tokenizer.tokenize(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_rows(df):\n",
    "    tokenized_headlines = df['headline'].apply(cleanup_and_tokenize_text).tolist()\n",
    "    tokenized_desc = df['short_description'].apply(cleanup_and_tokenize_text).tolist()\n",
    "\n",
    "    return [tokens1 + tokens2 for tokens1, tokens2 in zip(tokenized_headlines, tokenized_desc)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unigram_counts(rows, token_encoder):\n",
    "    # Flatten\n",
    "    tokens = [t for tokens in rows for t in tokens]\n",
    "    \n",
    "    counts = {}\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in counts:\n",
    "            counts[token] = 0\n",
    "        counts[token] += 1\n",
    "\n",
    "    return counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder_and_decoder(unigram_counts):\n",
    "    encoder = {t:i for i,t in enumerate(unigram_counts.keys())}\n",
    "    decoder = {i:t for t,i in encoder.items()}\n",
    "    \n",
    "    return encoder, decoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow_dataframe(encoded_token_rows, encoder, decoder):\n",
    "    bows = np.zeros((len(encoded_token_rows), len(encoder)))\n",
    "\n",
    "    for i, encoded_tokens in enumerate(encoded_token_rows):\n",
    "        for encoded in encoded_tokens:\n",
    "            bows[i, encoded] += 1\n",
    "    \n",
    "    df = pd.DataFrame(data=bows)\n",
    "    df.columns = [decoder[i] for i in range(len(decoder))]\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, token_encoder, log=True):\n",
    "    print_if_logging = lambda x: print(x) if log else None\n",
    "\n",
    "    print_if_logging(f'[2/8] Generating labels...')\n",
    "    labels = data['category']\n",
    "    \n",
    "    print_if_logging(f'[3/8] Tokenizing rows...')\n",
    "    token_rows = tokenize_rows(data)\n",
    "\n",
    "    print_if_logging('[4/8] Generating global unigram count ...')\n",
    "    unigram_counts = create_unigram_counts(token_rows)\n",
    "\n",
    "\n",
    "    print_if_logging('[5/8] Filtering out low-frequency words (only small dataset) ...')\n",
    "    if min_word_freq is not None:\n",
    "        token_rows = [[token if unigram_counts[token] > min_word_freq else __LOW_FREQ_TOKEN__ for token in tokens] for tokens in token_rows]\n",
    "        unigram_counts = create_unigram_counts(token_rows)\n",
    "    else:\n",
    "        print_if_logging(f'      Skipping low-frequency filtering')\n",
    "        \n",
    "\n",
    "    print_if_logging('[6/8] Create encoder / decoder ...')\n",
    "    encoder, decoder = create_encoder_and_decoder(unigram_counts)\n",
    "    \n",
    "\n",
    "    print_if_logging('[7/8] Encoding Token Rows ...')\n",
    "    encoded_token_rows = [[encoder[t] for t in tokens] for tokens in token_rows]\n",
    "\n",
    "    \n",
    "    print_if_logging('[8/8] Creating Bag Of Words DataFrame ...')\n",
    "    data_bow = create_bow_dataframe(encoded_token_rows, encoder, decoder)\n",
    "\n",
    "\n",
    "    return data_bow, labels, encoder, decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing large dataset...\n",
      "[1/7] Downsampling...\n",
      "Skipping Downsampling\n",
      "[2/8] Generating labels...\n",
      "[3/8] Tokenizing rows...\n",
      "[4/8] Generating global unigram count ...\n",
      "[5/8] Filtering out low-frequency words (only small dataset) ...\n",
      "Skipping low-frequency filtering\n",
      "[6/8] Create encoder / decoder ...\n",
      "[7/8] Encoding Token Rows ...\n",
      "[8/8] Creating Bag Of Words DataFrame ...\n",
      "Processing small dataset...\n",
      "[1/7] Downsampling...\n",
      "[2/8] Generating labels...\n",
      "[3/8] Tokenizing rows...\n",
      "[4/8] Generating global unigram count ...\n",
      "[5/8] Filtering out low-frequency words (only small dataset) ...\n",
      "[6/8] Create encoder / decoder ...\n",
      "[7/8] Encoding Token Rows ...\n",
      "[8/8] Creating Bag Of Words DataFrame ...\n",
      "Done!\n",
      "Ran in 0.45m\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print('Processing large dataset...')\n",
    "data_large, labels_large, encoder_large, decoder_large = process_data(data, min_word_freq=None, should_downsample=False)\n",
    "\n",
    "print('Processing small dataset...')\n",
    "data_small, labels_small, encoder_small, decoder_small = process_data(data, min_word_freq=MIN_WORD_FREQ, should_downsample=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Done!')\n",
    "print(f'Ran in {(end_time - start_time)/60:.02f}m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data_large) == 160607\n",
      "len(data_small) == 86654\n",
      "\n",
      "len(data_large.columns) == 101082\n",
      "len(data_small.columns) == 20959\n"
     ]
    }
   ],
   "source": [
    "print(f'len(data_large) == {len(data_large)}')\n",
    "print(f'len(data_small) == {len(data_small)}')\n",
    "print()\n",
    "print(f'len(data_large.columns) == {len(data_large.columns)}')\n",
    "print(f'len(data_small.columns) == {len(data_small.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: HERE I AM CREATING TORCH DATASET!\n",
    "\n",
    "class WorkTokenDataset(Dataset):\n",
    "    __TOKEN_UNK__ = '__TOKEN_UNK__'\n",
    "\n",
    "    __TOKEN_LOW_FREQ__ = '__TOKEN_LOW_FREQ__'\n",
    "    \n",
    "    __TOKEN_UNK__ = '__TOKEN_UNK__'\n",
    "\n",
    "\n",
    "    def __init__(self, data, embeddings, downsample_c=None, min_word_freq=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.embeddings = embeddings\n",
    "        self.downsample_c = downsample_c\n",
    "        self.min_word_freq = min_word_freq\n",
    "\n",
    "        self._is_prepared = False\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sub_data = self.data.iloc[idx]\n",
    "\n",
    "\n",
    "    def prepare(self):\n",
    "        tokenized_rows = tokenize_rows(self.data)\n",
    "        unigram_counts = create_unigram_counts(self.data)\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
