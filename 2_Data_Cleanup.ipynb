{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleanup\n",
    "\n",
    "The purpose of this notebook is to:\n",
    "\n",
    "1. Cleanup the data so it can be readily used by the models.\n",
    "2. Split the data into training and testing sets so the different models have a consistent benchmark.\n",
    "3. Create a mini version of the training set for models that are more computationally limited.\n",
    "\n",
    "**NOTE: This code uses pseudo-random operations, so running this script will generate different data on each run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset provided is malformed JSON. Need to fix up the JSON formatting\n",
    "# so that it can be ingested by pandas.\n",
    "\n",
    "with open('./data/News_Category_Dataset_v2.json') as file:\n",
    "    lines = file.readlines()\n",
    "    json = f'[{\",\".join(lines)}]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(json, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(df, c):\n",
    "    category_counts = df['category'].value_counts()\n",
    "    min_count = category_counts.min()\n",
    "\n",
    "    # Calculate the probability of keeping a row\n",
    "    # of a given category.\n",
    "    category_probs = (min_count / category_counts) ** (1/c)\n",
    "\n",
    "    # This is a series used to determine the probability that each\n",
    "    # row is kept. Each rows mask depends on its category.\n",
    "    prob_mask = np.zeros(len(df))\n",
    "\n",
    "    for i, category in enumerate(category_counts.index.tolist()):\n",
    "        category_prob = category_probs[i]\n",
    "        category_keep_mask = (df['category'] == category) * category_prob\n",
    "        prob_mask = prob_mask + category_keep_mask\n",
    "\n",
    "    keep_mask = np.random.rand(len(df)) <= prob_mask\n",
    "    \n",
    "    return df[keep_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = downsample(data, c=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.tokenize.regexp import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special token for tokens that occur MIN_WORD_FREQ or fewer times in the\n",
    "# entire corpus.\n",
    "__LOW_FREQ_TOKEN__ = '__LOW_FREQ_TOKEN__'\n",
    "\n",
    "MIN_WORD_FREQ = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_and_tokenize_text(text):\n",
    "    cleaned = ''.join([c for c in text if c not in string.punctuation]).lower()\n",
    "    return tokenizer.tokenize(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_rows(df):\n",
    "    tokenized_headlines = df['headline'].apply(cleanup_and_tokenize_text).tolist()\n",
    "    tokenized_desc = df['short_description'].apply(cleanup_and_tokenize_text).tolist()\n",
    "\n",
    "    return [tokens1 + tokens2 for tokens1, tokens2 in zip(tokenized_headlines, tokenized_desc)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unigram_counts(rows):\n",
    "    # Flatten\n",
    "    tokens = [t for tokens in rows for t in tokens]\n",
    "    \n",
    "    counts = {}\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in counts:\n",
    "            counts[token] = 0\n",
    "        counts[token] += 1\n",
    "\n",
    "    return counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder_and_decoder(unigram_counts):\n",
    "    encoder = {t:i for i,t in enumerate(unigram_counts.keys())}\n",
    "    decoder = {i:t for t,i in encoder.items()}\n",
    "    \n",
    "    return encoder, decoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow_dataframe(encoded_token_rows, encoder, decoder):\n",
    "    bows = np.zeros((len(encoded_token_rows), len(encoder)))\n",
    "\n",
    "    for i, encoded_tokens in enumerate(encoded_token_rows):\n",
    "        for encoded in encoded_tokens:\n",
    "            bows[i, encoded] += 1\n",
    "    \n",
    "    df = pd.DataFrame(data=bows)\n",
    "    df.columns = [decoder[i] for i in range(len(decoder))]\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/7] Tokenizing rows ...\n",
      "[2/7] Generating global unigram count ...\n",
      "[3/7] Filtering out low-frequency words ...\n",
      "[4/7] Re-computing unigram counts ...\n",
      "[5/7] Create encoder / decoder ...\n",
      "[6/7] Encoding Token Rows ...\n",
      "[7/7] Creating Bag Of Words DataFrame ...\n",
      "Done!\n",
      "Ran in 0.22m\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print('[1/7] Tokenizing rows ...')\n",
    "token_rows = tokenize_rows(data)\n",
    "\n",
    "print('[2/7] Generating global unigram count ...')\n",
    "unigram_counts = create_unigram_counts(token_rows)\n",
    "\n",
    "print('[3/7] Filtering out low-frequency words ...')\n",
    "token_rows = [[token if unigram_counts[token] > MIN_WORD_FREQ else __LOW_FREQ_TOKEN__ for token in tokens] for tokens in token_rows]\n",
    "\n",
    "print('[4/7] Re-computing unigram counts ...')\n",
    "unigram_counts = create_unigram_counts(token_rows)\n",
    "\n",
    "print('[5/7] Create encoder / decoder ...')\n",
    "encoder, decoder = create_encoder_and_decoder(unigram_counts)\n",
    "\n",
    "print('[6/7] Encoding Token Rows ...')\n",
    "encoded_token_rows = [[encoder[t] for t in tokens] for tokens in token_rows]\n",
    "\n",
    "print('[7/7] Creating Bag Of Words DataFrame ...')\n",
    "data_bow = create_bow_dataframe(encoded_token_rows, encoder, decoder)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Done!')\n",
    "print(f'Ran in {(end_time - start_time)/60:.02f}m')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_bow, labels, test_size=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaller version of the training data\n",
    "# for quicker training.\n",
    "\n",
    "sample_rate = 0.6\n",
    "sample_mask = np.random.rand(len(X_train)) < sample_rate\n",
    "\n",
    "X_train_mini = X_train[sample_mask]\n",
    "X_train_mini.reset_index(inplace=True)\n",
    "\n",
    "y_train_mini = y_train[sample_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] Saving training data...\n",
      "[2/3] Saving mini training data...\n",
      "[3/3] Saving test data...\n",
      "Done!\n",
      "Ran in 2.14m\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print(f'[1/3] Saving training data...')\n",
    "X_train.to_pickle('./data/train_data.pickle')\n",
    "y_train.to_pickle('./data/train_labels.pickle')\n",
    "\n",
    "print(f'[2/3] Saving mini training data...')\n",
    "X_train_mini.to_pickle('./data/train_data_mini.pickle')\n",
    "y_train_mini.to_pickle('./data/train_labels_mini.pickle')\n",
    "\n",
    "print(f'[3/3] Saving test data...')\n",
    "X_test.to_pickle('./data/test_data.pickle')\n",
    "y_test.to_pickle('./data/test_labels.pickle')\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Done!')\n",
    "print(f'Ran in {(end_time - start_time)/60:.02f}m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64722 samples in training set.\n",
      "39041 samples in mini training set.\n",
      "43148 samples in test set\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(X_train)} samples in training set.')\n",
    "\n",
    "print(f'{len(X_train_mini)} samples in mini training set.')\n",
    "\n",
    "print(f'{len(X_test)} samples in test set')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
