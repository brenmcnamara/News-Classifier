{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Data\n",
    "\n",
    "We will explore classifying this news data using a simple classifier: Logistic Regression.\n",
    "The Logistic Regression Algorithm will be given a bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset provided is malformed JSON. Need to fix up the JSON formatting\n",
    "# so that it can be ingested by pandas.\n",
    "\n",
    "with open('./data/News_Category_Dataset_v2.json') as file:\n",
    "    lines = file.readlines()\n",
    "    json = f'[{\",\".join(lines)}]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(json, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to limitation in compute, I need to downsample the data. The goal for downsampling will be to make sure there are enough samples of different types of data. Each row is kept with a probability that is inversly proportional to number of rows that exist in the data frame with the same label. By doing this, we downsample from more common categories more aggresively than less common categories. A parameter, C, will be used to configure the probability downsampling. A C value of 1 means all categories will have the same expected number of rows with larger values of C mean that downsampling happens less (as C approaches infinity, downsampling does not happen at all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(df, c):\n",
    "    category_counts = df['category'].value_counts()\n",
    "    min_count = category_counts.min()\n",
    "\n",
    "    # Calculate the probability of keeping a row\n",
    "    # of a given category.\n",
    "    category_probs = (min_count / category_counts) ** (1/c)\n",
    "\n",
    "    # This is a series used to determine the probability that each\n",
    "    # row is kept. Each rows mask depends on its category.\n",
    "    prob_mask = np.zeros(len(df))\n",
    "\n",
    "    for i, category in enumerate(category_counts.index.tolist()):\n",
    "        category_prob = category_probs[i]\n",
    "        category_keep_mask = (df['category'] == category) * category_prob\n",
    "        prob_mask = prob_mask + category_keep_mask\n",
    "\n",
    "    keep_mask = np.random.rand(len(df)) <= prob_mask\n",
    "    \n",
    "    return df[keep_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = downsample(data, c=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>IMPACT</td>\n",
       "      <td>With Its Way Of Life At Risk, This Remote Oyst...</td>\n",
       "      <td>Karen Pinchin</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/remote-oy...</td>\n",
       "      <td>The revolution is coming to rural New Brunswick.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>Ireland Votes To Repeal Abortion Amendment In ...</td>\n",
       "      <td>Laura Bassett</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/results-f...</td>\n",
       "      <td>Irish women will no longer have to travel to t...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>WEIRD NEWS</td>\n",
       "      <td>Weird Father's Day Gifts Your Dad Doesn't Know...</td>\n",
       "      <td>David Moye</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/weird-fat...</td>\n",
       "      <td>Why buy a boring tie when you can give him tes...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                                           headline  \\\n",
       "0           CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "4   ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "12         IMPACT  With Its Way Of Life At Risk, This Remote Oyst...   \n",
       "17       POLITICS  Ireland Votes To Repeal Abortion Amendment In ...   \n",
       "20     WEIRD NEWS  Weird Father's Day Gifts Your Dad Doesn't Know...   \n",
       "\n",
       "            authors                                               link  \\\n",
       "0   Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "4        Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "12    Karen Pinchin  https://www.huffingtonpost.com/entry/remote-oy...   \n",
       "17    Laura Bassett  https://www.huffingtonpost.com/entry/results-f...   \n",
       "20       David Moye  https://www.huffingtonpost.com/entry/weird-fat...   \n",
       "\n",
       "                                    short_description       date  \n",
       "0   She left her husband. He killed their children... 2018-05-26  \n",
       "4   The \"Dietland\" actress said using the bags is ... 2018-05-26  \n",
       "12   The revolution is coming to rural New Brunswick. 2018-05-26  \n",
       "17  Irish women will no longer have to travel to t... 2018-05-26  \n",
       "20  Why buy a boring tie when you can give him tes... 2018-05-26  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POLITICS          5810\n",
       "WELLNESS          4256\n",
       "ENTERTAINMENT     3883\n",
       "TRAVEL            3131\n",
       "STYLE & BEAUTY    3123\n",
       "PARENTING         2948\n",
       "HEALTHY LIVING    2650\n",
       "QUEER VOICES      2555\n",
       "FOOD & DRINK      2540\n",
       "BUSINESS          2418\n",
       "COMEDY            2302\n",
       "SPORTS            2229\n",
       "BLACK VOICES      2204\n",
       "HOME & LIVING     2044\n",
       "PARENTS           1971\n",
       "WOMEN             1923\n",
       "THE WORLDPOST     1914\n",
       "WEDDINGS          1905\n",
       "IMPACT            1868\n",
       "DIVORCE           1844\n",
       "CRIME             1838\n",
       "MEDIA             1696\n",
       "WEIRD NEWS        1644\n",
       "RELIGION          1617\n",
       "GREEN             1613\n",
       "WORLDPOST         1601\n",
       "STYLE             1543\n",
       "TASTE             1481\n",
       "SCIENCE           1476\n",
       "WORLD NEWS        1462\n",
       "TECH              1431\n",
       "MONEY             1311\n",
       "ARTS              1215\n",
       "GOOD NEWS         1199\n",
       "FIFTY             1187\n",
       "ENVIRONMENT       1177\n",
       "ARTS & CULTURE    1156\n",
       "COLLEGE           1077\n",
       "LATINO VOICES     1064\n",
       "CULTURE & ARTS    1018\n",
       "EDUCATION         1004\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dominant class is Politics. What portion of news articles are classified as Politics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7.06%'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{float((data[\"category\"] == \"POLITICS\").sum()) / len(data[\"category\"]) * 100:.02f}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as a baseline, we would expect our model to have an accuracy of at least as good as *16%*, which would be the equivalent of classifying every news article as Politics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     82328\n",
       "unique    16679\n",
       "top            \n",
       "freq      15252\n",
       "Name: authors, dtype: object"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['authors'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                                          15252\n",
       "Lee Moran                                                                                                  1094\n",
       "Ron Dicker                                                                                                  867\n",
       "Reuters, Reuters                                                                                            651\n",
       "Ed Mazza                                                                                                    571\n",
       "                                                                                                          ...  \n",
       "By Michelle Nichols, Reuters                                                                                  1\n",
       "Karen E. Quinones Miller, ContributorJournalist, Best-Selling Author, Activist, An All-around Angry...        1\n",
       "Hale Dwoskin, Contributor\\nAuthor, 'The Sedona Method'                                                        1\n",
       "Amelia, Contributor\\nMother and breadwinner                                                                   1\n",
       "Amine Chouaieb, Contributor\\nEntrepreneur                                                                     1\n",
       "Name: authors, Length: 16679, dtype: int64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['authors'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large portion of articles have missing authors. It would be good to get a sense of the distribution of articles written by repeat authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    16678.000000\n",
       "mean         4.021825\n",
       "std         22.699956\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          1.000000\n",
       "75%          2.000000\n",
       "max       1094.000000\n",
       "Name: authors, dtype: float64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_dist = data['authors'].value_counts()\n",
    "authors_dist = authors_dist.drop('')\n",
    "\n",
    "authors_dist.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a long tail of single-article authors. What portion of articles contain an author? What portion of articles contain a repeating author?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.47% of articles contain authors.\n",
      "68.15 articles contain repeat authors.\n"
     ]
    }
   ],
   "source": [
    "repeat_authors = authors_dist[authors_dist > 1].index.values\n",
    "\n",
    "count_articles = len(data)\n",
    "count_articles_with_authors = len(data[data['authors'] != ''])\n",
    "count_articles_with_repeat_authors = len(data[data['authors'].isin(repeat_authors)])\n",
    "\n",
    "print(f'{float(count_articles_with_authors) / count_articles * 100:.02f}% of articles contain authors.')\n",
    "print(f'{float(count_articles_with_repeat_authors) / count_articles * 100:.02f} articles contain repeat authors.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing and Exploring Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.tokenize.regexp import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_and_tokenize_text(text):\n",
    "    cleaned = ''.join([c for c in text if c not in string.punctuation]).lower()\n",
    "    return tokenizer.tokenize(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_rows(df):\n",
    "    tokenized_headlines = df['headline'].apply(cleanup_and_tokenize_text).tolist()\n",
    "    tokenized_desc = df['short_description'].apply(cleanup_and_tokenize_text).tolist()\n",
    "\n",
    "    return [tokens1 + tokens2 for tokens1, tokens2 in zip(tokenized_headlines, tokenized_desc)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unigram_counts(rows):\n",
    "    # Flatten\n",
    "    tokens = [t for tokens in rows for t in tokens]\n",
    "    \n",
    "    counts = {}\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in counts:\n",
    "            counts[token] = 0\n",
    "        counts[token] += 1\n",
    "\n",
    "    return counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder_and_decoder(unigram_counts):\n",
    "    encoder = {t:i for i,t in enumerate(unigram_counts.keys())}\n",
    "    decoder = {i:t for t,i in encoder.items()}\n",
    "    \n",
    "    return encoder, decoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow_dataframe(encoded_token_rows, encoder, decoder):\n",
    "    bows = np.zeros((len(encoded_token_rows), len(encoder)))\n",
    "\n",
    "    for i, encoded_tokens in enumerate(encoded_token_rows):\n",
    "        for encoded in encoded_tokens:\n",
    "            bows[i, encoded] += 1\n",
    "    \n",
    "    df = pd.DataFrame(data=bows)\n",
    "    df.columns = [decoder[i] for i in range(len(decoder))]\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Tokenizing rows...\n",
      "[2/2] Generating global unigram count...\n",
      "Done!\n",
      "Ran in 0.07m\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print('[1/2] Tokenizing rows...')\n",
    "token_rows = tokenize_rows(data)\n",
    "\n",
    "print('[2/2] Generating global unigram count...')\n",
    "unigram_counts = create_unigram_counts(token_rows)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Done!')\n",
    "print(f'Ran in {(end_time - start_time)/60:.02f}m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 75412 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(unigram_counts)} unique tokens.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Low-Frequency Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_WORD_FREQ = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 54986 low count tokens.\n"
     ]
    }
   ],
   "source": [
    "low_count_tokens = [t for t,c in unigram_counts.items() if c <= MIN_WORD_FREQ]\n",
    "\n",
    "print(f'There are {len(low_count_tokens)} low count tokens.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than three-forths of our vocabulary consists of words that show up fewer than `MIN_WORD_FREQ` times throughout the corpus. These words could slow down learning dramatically while not providing much signal. Will marginalize these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special token for tokens that occur MIN_WORD_FREQ or fewer times in the\n",
    "# entire corpus.\n",
    "__LOW_FREQ_TOKEN__ = '__LOW_FREQ_TOKEN__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Filtering out low-frequency words...\n",
      "[2/2] Re-computing unigram counts...\n",
      "Done!\n",
      "Ran in 0.03m\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print(f'[1/2] Filtering out low-frequency words...')\n",
    "token_rows = [[token if unigram_counts[token] > 10 else __LOW_FREQ_TOKEN__ for token in tokens] for tokens in token_rows]\n",
    "\n",
    "print(f'[2/2] Re-computing unigram counts...')\n",
    "unigram_counts = create_unigram_counts(token_rows)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Done!')\n",
    "print(f'Ran in {(end_time - start_time)/60:.02f}m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13996 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(unigram_counts)} unique tokens.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] Create encoder / decoder...\n",
      "[2/3] Encoding Token Rows...\n",
      "[3/3] Creating Bag Of Words DataFrame...\n",
      "Done!\n",
      "Ran in 0.08m\n"
     ]
    }
   ],
   "source": [
    "# Fully process the text in the data frame to a one-hot vector\n",
    "# bag-of-words representation.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print('[1/3] Create encoder / decoder...')\n",
    "encoder, decoder = create_encoder_and_decoder(unigram_counts)\n",
    "\n",
    "print('[2/3] Encoding Token Rows...')\n",
    "encoded_token_rows = [[encoder[t] for t in tokens] for tokens in token_rows]\n",
    "\n",
    "print('[3/3] Creating Bag Of Words DataFrame...')\n",
    "data_bow = create_bow_dataframe(encoded_token_rows, encoder, decoder)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Done!')\n",
    "print(f'Ran in {(end_time - start_time)/60:.02f}m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>there</th>\n",
       "      <th>were</th>\n",
       "      <th>2</th>\n",
       "      <th>mass</th>\n",
       "      <th>shootings</th>\n",
       "      <th>in</th>\n",
       "      <th>texas</th>\n",
       "      <th>last</th>\n",
       "      <th>week</th>\n",
       "      <th>but</th>\n",
       "      <th>...</th>\n",
       "      <th>vases</th>\n",
       "      <th>leann</th>\n",
       "      <th>printable</th>\n",
       "      <th>g8</th>\n",
       "      <th>trierweiler</th>\n",
       "      <th>dolomites</th>\n",
       "      <th>jubilee</th>\n",
       "      <th>stylelist</th>\n",
       "      <th>donnas</th>\n",
       "      <th>psychometer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13996 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   there  were    2  mass  shootings   in  texas  last  week  but  ...  vases  \\\n",
       "0    1.0   1.0  1.0   1.0        1.0  2.0    1.0   1.0   1.0  1.0  ...    0.0   \n",
       "1    0.0   0.0  0.0   0.0        0.0  0.0    0.0   0.0   0.0  0.0  ...    0.0   \n",
       "2    0.0   0.0  0.0   0.0        0.0  1.0    0.0   0.0   0.0  0.0  ...    0.0   \n",
       "3    0.0   0.0  0.0   0.0        0.0  1.0    0.0   0.0   0.0  0.0  ...    0.0   \n",
       "4    0.0   0.0  0.0   0.0        0.0  0.0    0.0   0.0   0.0  1.0  ...    0.0   \n",
       "\n",
       "   leann  printable   g8  trierweiler  dolomites  jubilee  stylelist  donnas  \\\n",
       "0    0.0        0.0  0.0          0.0        0.0      0.0        0.0     0.0   \n",
       "1    0.0        0.0  0.0          0.0        0.0      0.0        0.0     0.0   \n",
       "2    0.0        0.0  0.0          0.0        0.0      0.0        0.0     0.0   \n",
       "3    0.0        0.0  0.0          0.0        0.0      0.0        0.0     0.0   \n",
       "4    0.0        0.0  0.0          0.0        0.0      0.0        0.0     0.0   \n",
       "\n",
       "   psychometer  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          0.0  \n",
       "\n",
       "[5 rows x 13996 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Vocab Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13996"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_most_frequent(unigram_counts, k):\n",
    "    top_words = []\n",
    "\n",
    "    # The \"candidate\" is the word in the list that is\n",
    "    # next up to get replaced if we find a better word.\n",
    "    candidate_count = 0\n",
    "    candidate_index = -1\n",
    "\n",
    "    # We want to support k most and k least frequent words.\n",
    "    pos_k = k if k >= 0 else -k\n",
    "    min_or_max = min if k >= 0 else max\n",
    "\n",
    "    for word, count in unigram_counts.items():\n",
    "\n",
    "        if len(top_words) < pos_k or min_or_max(count, candidate_count) == candidate_count:\n",
    "            top_words.append(word)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if len(top_words) > pos_k:\n",
    "            # Need to remove the shortest word.\n",
    "            del top_words[candidate_index]\n",
    "            \n",
    "        counts = [unigram_counts[w] for w in top_words]\n",
    "        candidate_count = min_or_max(counts)\n",
    "        candidate_index = counts.index(candidate_count)\n",
    "        \n",
    "    return top_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " '__LOW_FREQ_TOKEN__',\n",
       " 'to',\n",
       " 'the',\n",
       " 'is',\n",
       " 'a',\n",
       " 'of',\n",
       " 'for',\n",
       " 'and',\n",
       " 'that']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most Frequent Words\n",
    "k_most_frequent(unigram_counts, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['divorcee',\n",
       " 'fabrics',\n",
       " 'cm',\n",
       " 'summery',\n",
       " 'huffpostbeauty',\n",
       " 'gwist',\n",
       " 'printable',\n",
       " 'trierweiler',\n",
       " 'donnas',\n",
       " 'psychometer']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Least Frequent Words\n",
    "k_most_frequent(unigram_counts, k=-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
