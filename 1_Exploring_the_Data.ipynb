{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Data\n",
    "\n",
    "We will explore classifying this news data using a simple classifier: Logistic Regression.\n",
    "The Logistic Regression Algorithm will be given a bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset provided is malformed JSON. Need to fix up the JSON formatting\n",
    "# so that it can be ingested by pandas.\n",
    "\n",
    "with open('./News_Category_Dataset_v2.json') as file:\n",
    "    lines = file.readlines()\n",
    "    json = f'[{\",\".join(lines)}]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_json(json, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description       date  \n",
       "0  She left her husband. He killed their children... 2018-05-26  \n",
       "1                           Of course it has a song. 2018-05-26  \n",
       "2  The actor and his longtime girlfriend Anna Ebe... 2018-05-26  \n",
       "3  The actor gives Dems an ass-kicking for not fi... 2018-05-26  \n",
       "4  The \"Dietland\" actress said using the bags is ... 2018-05-26  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POLITICS          32739\n",
       "WELLNESS          17827\n",
       "ENTERTAINMENT     16058\n",
       "TRAVEL             9887\n",
       "STYLE & BEAUTY     9649\n",
       "PARENTING          8677\n",
       "HEALTHY LIVING     6694\n",
       "QUEER VOICES       6314\n",
       "FOOD & DRINK       6226\n",
       "BUSINESS           5937\n",
       "COMEDY             5175\n",
       "SPORTS             4884\n",
       "BLACK VOICES       4528\n",
       "HOME & LIVING      4195\n",
       "PARENTS            3955\n",
       "THE WORLDPOST      3664\n",
       "WEDDINGS           3651\n",
       "WOMEN              3490\n",
       "IMPACT             3459\n",
       "DIVORCE            3426\n",
       "CRIME              3405\n",
       "MEDIA              2815\n",
       "WEIRD NEWS         2670\n",
       "GREEN              2622\n",
       "WORLDPOST          2579\n",
       "RELIGION           2556\n",
       "STYLE              2254\n",
       "SCIENCE            2178\n",
       "WORLD NEWS         2177\n",
       "TASTE              2096\n",
       "TECH               2082\n",
       "MONEY              1707\n",
       "ARTS               1509\n",
       "FIFTY              1401\n",
       "GOOD NEWS          1398\n",
       "ARTS & CULTURE     1339\n",
       "ENVIRONMENT        1323\n",
       "COLLEGE            1144\n",
       "LATINO VOICES      1129\n",
       "CULTURE & ARTS     1030\n",
       "EDUCATION          1004\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dominant class is Politics. What portion of news articles are classified as Politics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16.30%'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{float((data_raw[\"category\"] == \"POLITICS\").sum()) / len(data_raw[\"category\"]) * 100:.02f}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as a baseline, we would expect our model to have an accuracy of at least as good as *16%*, which would be the equivalent of classifying every news article as Politics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     200853\n",
       "unique     27993\n",
       "top             \n",
       "freq       36620\n",
       "Name: authors, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw['authors'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                                 36620\n",
       "Lee Moran                                                                                         2423\n",
       "Ron Dicker                                                                                        1913\n",
       "Reuters, Reuters                                                                                  1562\n",
       "Ed Mazza                                                                                          1322\n",
       "                                                                                                 ...  \n",
       "Bonnie St. John, ContributorOlympic Ski Medalist, Amputee, Rhodes Scholar, former White Ho...        1\n",
       "Basil Soper, ContributorTrans writer, intersectional activist, double cancer, and anim...            1\n",
       "Jacqueline Herrera, Contributor\\nCo-founder, Kitechild                                               1\n",
       "Irwin Zalkin, ContributorAttorney                                                                    1\n",
       "Anna Rosen-Birch, ContributorHigh school student                                                     1\n",
       "Name: authors, Length: 27993, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw['authors'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large portion of articles have missing authors. It would be good to get a sense of the distribution of articles written by repeat authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27992.000000\n",
       "mean         5.867141\n",
       "std         41.929860\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          1.000000\n",
       "75%          3.000000\n",
       "max       2423.000000\n",
       "Name: authors, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_dist = data_raw['authors'].value_counts()\n",
    "authors_dist = authors_dist.drop('')\n",
    "\n",
    "authors_dist.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a long tail of single-article authors. What portion of articles contain an author? What portion of articles contain a repeating author?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.77% of articles contain authors.\n",
      "73.45 articles contain repeat authors.\n"
     ]
    }
   ],
   "source": [
    "repeat_authors = authors_dist[authors_dist > 1].index.values\n",
    "\n",
    "count_articles = len(data_raw)\n",
    "count_articles_with_authors = len(data_raw[data_raw['authors'] != ''])\n",
    "count_articles_with_repeat_authors = len(data_raw[data_raw['authors'].isin(repeat_authors)])\n",
    "\n",
    "print(f'{float(count_articles_with_authors) / count_articles * 100:.02f}% of articles contain authors.')\n",
    "print(f'{float(count_articles_with_repeat_authors) / count_articles * 100:.02f} articles contain repeat authors.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing and Exploring Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.tokenize.regexp import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_and_tokenize_text(text):\n",
    "    cleaned = ''.join([c for c in text if c not in string.punctuation]).lower()\n",
    "    return tokenizer.tokenize(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_rows(df):\n",
    "    tokenized_headlines = df['headline'].apply(cleanup_and_tokenize_text).tolist()\n",
    "    tokenized_desc = df['short_description'].apply(cleanup_and_tokenize_text).tolist()\n",
    "\n",
    "    return [tokens1 + tokens2 for tokens1, tokens2 in zip(tokenized_headlines, tokenized_desc)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unigram_counts(rows):\n",
    "    # Flatten\n",
    "    tokens = [t for tokens in rows for t in tokens]\n",
    "    \n",
    "    counts = {}\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in counts:\n",
    "            counts[token] = 0\n",
    "        counts[token] += 1\n",
    "\n",
    "    return counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder_and_decoder(unigram_counts):\n",
    "    encoder = {t:i for i,t in enumerate(unigram_counts.keys())}\n",
    "    decoder = {i:t for t,i in encoder.items()}\n",
    "    \n",
    "    return encoder, decoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow_dataframe(encoded_token_rows, encoder, decoder):\n",
    "    bows = np.zeros((len(encoded_token_rows), len(encoder)))\n",
    "\n",
    "    for i, encoded_tokens in enumerate(encoded_token_rows):\n",
    "        for encoded in encoded_tokens:\n",
    "            bows[i, encoded] += 1\n",
    "    \n",
    "    df = pd.DataFrame(data=bows)\n",
    "    df.columns = [decoder[i] for i in range(len(decoder))]\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Tokenizing rows...\n",
      "[2/2] Generating global unigram count...\n",
      "Done!\n",
      "Ran in 0.20m\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print('[1/2] Tokenizing rows...')\n",
    "token_rows = tokenize_rows(data_raw)\n",
    "\n",
    "print('[2/2] Generating global unigram count...')\n",
    "unigram_counts = create_unigram_counts(token_rows)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Done!')\n",
    "print(f'Ran in {(end_time - start_time)/60:.02f}m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 112586 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(unigram_counts)} unique tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 80763 low count words.\n"
     ]
    }
   ],
   "source": [
    "low_count_words = [w for w,c in unigram_counts.items() if c <= 5]\n",
    "\n",
    "print(f'There are {len(low_count_words)} low count words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Low-Frequency Words\n",
    "\n",
    "More than two-thirds of our vocabulary consists of words that show up fewer than 5 times throughout the corpus. These words could slow down learning dramatically while not providing much signal. Will marginalize these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special token for tokens that occur 5 or fewer times in the\n",
    "# entire corpus.\n",
    "__LOW_FREQ_TOKEN__ = '__LOW_FREQ_TOKEN__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Filtering out low-frequency words...\n",
      "[2/2] Re-computing unigram counts...\n",
      "Done!\n",
      "Ran in 0.05m\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print(f'[1/2] Filtering out low-frequency words...')\n",
    "token_rows = [[token if unigram_counts[token] > 5 else __LOW_FREQ_TOKEN__ for token in tokens] for tokens in token_rows]\n",
    "\n",
    "print(f'[2/2] Re-computing unigram counts...')\n",
    "unigram_counts = create_unigram_counts(token_rows)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Done!')\n",
    "print(f'Ran in {(end_time - start_time)/60:.02f}m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 31824 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(unigram_counts)} unique tokens.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] Create encoder / decoder...\n",
      "[2/3] Encoding Token Rows...\n",
      "[3/3] Creating Bag Of Words DataFrame...\n",
      "Done!\n",
      "Ran in 0.23m\n"
     ]
    }
   ],
   "source": [
    "# Fully process the text in the data frame to a one-hot vector\n",
    "# bag-of-words representation.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print('[1/3] Create encoder / decoder...')\n",
    "encoder, decoder = create_encoder_and_decoder(unigram_counts)\n",
    "\n",
    "print('[2/3] Encoding Token Rows...')\n",
    "encoded_token_rows = [[encoder[t] for t in tokens] for tokens in token_rows]\n",
    "\n",
    "print('[3/3] Creating Bag Of Words DataFrame...')\n",
    "data_bow = create_bow_dataframe(encoded_token_rows, encoder, decoder)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Done!')\n",
    "print(f'Ran in {(end_time - start_time)/60:.02f}m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>there</th>\n",
       "      <th>were</th>\n",
       "      <th>2</th>\n",
       "      <th>mass</th>\n",
       "      <th>shootings</th>\n",
       "      <th>in</th>\n",
       "      <th>texas</th>\n",
       "      <th>last</th>\n",
       "      <th>week</th>\n",
       "      <th>but</th>\n",
       "      <th>...</th>\n",
       "      <th>oosthuizen</th>\n",
       "      <th>sozzani</th>\n",
       "      <th>flajnik</th>\n",
       "      <th>alber</th>\n",
       "      <th>elbaz</th>\n",
       "      <th>qnexa</th>\n",
       "      <th>garance</th>\n",
       "      <th>mengelt</th>\n",
       "      <th>collegehoopsnet</th>\n",
       "      <th>xlvi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31824 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   there  were    2  mass  shootings   in  texas  last  week  but  ...  \\\n",
       "0    1.0   1.0  1.0   1.0        1.0  2.0    1.0   1.0   1.0  1.0  ...   \n",
       "1    0.0   0.0  0.0   0.0        0.0  0.0    0.0   0.0   0.0  0.0  ...   \n",
       "2    0.0   0.0  0.0   0.0        0.0  1.0    0.0   0.0   0.0  0.0  ...   \n",
       "3    0.0   0.0  0.0   0.0        0.0  1.0    0.0   0.0   0.0  0.0  ...   \n",
       "4    0.0   0.0  0.0   0.0        0.0  0.0    0.0   0.0   0.0  0.0  ...   \n",
       "\n",
       "   oosthuizen  sozzani  flajnik  alber  elbaz  qnexa  garance  mengelt  \\\n",
       "0         0.0      0.0      0.0    0.0    0.0    0.0      0.0      0.0   \n",
       "1         0.0      0.0      0.0    0.0    0.0    0.0      0.0      0.0   \n",
       "2         0.0      0.0      0.0    0.0    0.0    0.0      0.0      0.0   \n",
       "3         0.0      0.0      0.0    0.0    0.0    0.0      0.0      0.0   \n",
       "4         0.0      0.0      0.0    0.0    0.0    0.0      0.0      0.0   \n",
       "\n",
       "   collegehoopsnet  xlvi  \n",
       "0              0.0   0.0  \n",
       "1              0.0   0.0  \n",
       "2              0.0   0.0  \n",
       "3              0.0   0.0  \n",
       "4              0.0   0.0  \n",
       "\n",
       "[5 rows x 31824 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Vocab Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112586"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_most_frequent(unigram_counts, k):\n",
    "    top_words = []\n",
    "\n",
    "    # The \"candidate\" is the word in the list that is\n",
    "    # next up to get replaced if we find a better word.\n",
    "    candidate_count = 0\n",
    "    candidate_index = -1\n",
    "\n",
    "    # We want to support k most and k least frequent words.\n",
    "    pos_k = k if k >= 0 else -k\n",
    "    min_or_max = min if k >= 0 else max\n",
    "\n",
    "    for word, count in unigram_counts.items():\n",
    "\n",
    "        if len(top_words) < pos_k or min_or_max(count, candidate_count) == candidate_count:\n",
    "            top_words.append(word)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if len(top_words) > pos_k:\n",
    "            # Need to remove the shortest word.\n",
    "            del top_words[candidate_index]\n",
    "            \n",
    "        counts = [unigram_counts[w] for w in top_words]\n",
    "        candidate_count = min_or_max(counts)\n",
    "        candidate_index = counts.index(candidate_count)\n",
    "        \n",
    "    return top_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'and',\n",
       " 'for',\n",
       " 'the',\n",
       " 'of',\n",
       " 'a',\n",
       " '__LOW_FREQ_TOKEN__',\n",
       " 'to',\n",
       " 'is',\n",
       " 'that']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most Frequent Words\n",
    "k_most_frequent(unigram_counts, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trop',\n",
       " 'clementis',\n",
       " 'krentcil',\n",
       " 'keiko',\n",
       " 'prettify',\n",
       " 'flajnik',\n",
       " 'alber',\n",
       " 'qnexa',\n",
       " 'mengelt',\n",
       " 'collegehoopsnet']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Least Frequent Words\n",
    "k_most_frequent(unigram_counts, k=-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
